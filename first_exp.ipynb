{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(data_loader, index):\n",
    "    images, labels = next(iter(data_loader))\n",
    "    if index >= len(images):\n",
    "        print(f\"Index {index} is out of range. Maximum batch size is {len(images)}.\")\n",
    "        return\n",
    "\n",
    "    image = images[index]\n",
    "    label = labels[index].item()\n",
    "\n",
    "    # Unnormalize the image for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image = image * std + mean\n",
    "    image = image.clamp(0, 1)  # Clip values to [0, 1]\n",
    "\n",
    "    # Convert the image tensor to a numpy array\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Display the image and label\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loaders(size_tuple, augmented_count):\n",
    "    # Define the path to the root directory containing the class folders\n",
    "    root_dir = \"data\"\n",
    "\n",
    "    # Define a mapping from folder names to class numbers\n",
    "    class_mapping = {\n",
    "        'barszcz': 1,\n",
    "        'bigos': 2,\n",
    "        'Kutia': 3,\n",
    "        'makowiec': 4,\n",
    "        'piernik': 5,\n",
    "        'pierogi': 6,\n",
    "        'sernik': 7,\n",
    "        'grzybowa': 8\n",
    "    }\n",
    "\n",
    "    batch_size = 32\n",
    "    validation_split = 0.2  # Proportion of data for validation\n",
    "\n",
    "    # Data Augmentation and Transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size_tuple),\n",
    "        transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "        transforms.RandomCrop(size=(size_tuple[0]-20, size_tuple[1]-20), padding=20),  # Random crop\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=.3, hue=.1) ,\n",
    "        transforms.RandomVerticalFlip(0.1),\n",
    "        transforms.Resize(size_tuple),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "\n",
    "    # Load the original dataset\n",
    "    original_dataset = datasets.ImageFolder(root=root_dir, transform=transforms.Compose([\n",
    "        transforms.Resize(size_tuple),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ]))\n",
    "\n",
    "    original_dataset.class_to_idx = {k: class_mapping[k] for k in original_dataset.class_to_idx.keys()}\n",
    "\n",
    "\n",
    "    for _ in range(augmented_count):\n",
    "        # Load the original dataset\n",
    "        augmented_dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "        augmented_dataset.class_to_idx = {k: class_mapping[k] for k in augmented_dataset.class_to_idx.keys()}\n",
    "\n",
    "        # Combine original and augmented datasets\n",
    "        original_dataset = ConcatDataset([original_dataset, augmented_dataset])\n",
    "\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    dataset_size = len(original_dataset)\n",
    "    validation_size = int(validation_split * dataset_size)\n",
    "    train_size = dataset_size - validation_size\n",
    "    train_dataset, validation_dataset = random_split(original_dataset, [train_size, validation_size])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return (train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import torchvision.models as models\n",
    "import timm  # for Vision Transformer\n",
    "\n",
    "class EnsembleTrainer:\n",
    "    def __init__(self, num_classes, device, patience=10, min_delta=0.01, max_epochs=50):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        self.model_weights = dict()\n",
    "\n",
    "        self.models = {\n",
    "            #'swin_t': self._prepare_swin_t(num_classes),\n",
    "            # 'efficientnet_b0': self._prepare_efficientnet_b0(num_classes),\n",
    "            # 'resnet_18': self._prepare_resnet_18(num_classes),\n",
    "            'mobilenet_v3_large': self._prepare_mobilenet_v3_large(num_classes),\n",
    "            'vit_tiny': self._prepare_vit_tiny(num_classes)\n",
    "        }\n",
    "        \n",
    "        # Optimizers for each model\n",
    "        self.optimizers = {\n",
    "            name: optim.AdamW(model.parameters()) \n",
    "            for name, model in self.models.items()\n",
    "        }\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.results = {\n",
    "            name: {\n",
    "                'train_losses': [],\n",
    "                'val_losses': [],\n",
    "                'f1_scores': [],\n",
    "                'best_val_f1': 0,\n",
    "                'patience_counter': 0\n",
    "            } for name in self.models.keys()\n",
    "        }\n",
    "\n",
    "\n",
    "    def _prepare_efficientnet_b0(self, num_classes):\n",
    "        model = models.efficientnet_b0(weights='DEFAULT')\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _prepare_resnet_18(self, num_classes):\n",
    "        model = models.resnet18(weights='DEFAULT')\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def _prepare_mobilenet_v3_large(self, num_classes):\n",
    "        model = models.mobilenet_v3_large(weights='IMAGENET1K_V2')\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _prepare_vit_tiny(self, num_classes):\n",
    "        model = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def _prepare_swin_t(self, num_classes):\n",
    "        model = models.swin_t(weights='DEFAULT')\n",
    "        if hasattr(model, 'head'):\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        else:\n",
    "            model.fc = nn.Linear(model.num_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "        \n",
    "\n",
    "    def train_single_model(self, model, optimizer, train_loader, validation_loader, model_name):\n",
    "        for epoch in range(1, self.max_epochs + 1):  # Your original epoch range\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Training phase\n",
    "            model.train()\n",
    "            epoch_loss_train = 0\n",
    "            running_loss = 0\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss_train += loss.item()\n",
    "                \n",
    "                if i % 20 == 19:\n",
    "                    print(f\"{model_name} - Current time: {round(time.time() - current_time, 0)} s, \"\n",
    "                          f\"epoch: {epoch}/50, minibatch: {i + 1:5d}/{len(train_loader)}, \"\n",
    "                          f\"running loss: {running_loss / 500:.3f}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            epoch_loss_val = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in validation_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    epoch_loss_val += loss.item()\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Compute metrics\n",
    "            avg_train_loss = epoch_loss_train / len(train_loader)\n",
    "            avg_val_loss = epoch_loss_val / len(validation_loader)\n",
    "            val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "            # Update results\n",
    "            results = self.results[model_name]\n",
    "            results['train_losses'].append(avg_train_loss)\n",
    "            results['val_losses'].append(avg_val_loss)\n",
    "            results['f1_scores'].append(val_f1)\n",
    "\n",
    "            print(f\"{model_name} - Epoch {epoch}: \"\n",
    "                  f\"Train Loss = {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss = {avg_val_loss:.4f}, \"\n",
    "                  f\"Val F1 = {val_f1:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_f1 > results['best_val_f1'] + self.min_delta:\n",
    "                results['best_val_f1'] = val_f1\n",
    "                results['patience_counter'] = 0\n",
    "                \n",
    "                # Save best model\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': results['train_losses'],\n",
    "                    'val_loss': results['val_losses'],\n",
    "                    'f1_metric_val': results['f1_scores'],\n",
    "                    'best_val_f1': results['best_val_f1'],\n",
    "                }, f'models/{model_name}_best_f1.pth')\n",
    "                self.model_weights[model_name] = val_f1\n",
    "            else:\n",
    "                results['patience_counter'] += 1\n",
    "\n",
    "            if results['patience_counter'] >= self.patience:\n",
    "                print(f\"{model_name} - Early stopping triggered at epoch {epoch}, best f1 score {results['f1_scores'][-11]}\")\n",
    "                break\n",
    "\n",
    "    def train_ensemble(self, train_loader, validation_loader):\n",
    "        # Train each model\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nTraining {name}\")\n",
    "            self.train_single_model(model, self.optimizers[name], \n",
    "                                    train_loader, validation_loader, name)\n",
    "\n",
    "    def ensemble_predict(self, dataloader):\n",
    "        # Weighted voting ensemble prediction\n",
    "        all_ensemble_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Normalizacja wag, jeśli nie sumują się do 1\n",
    "        total_weight = sum(self.model_weights.values())\n",
    "        model_weights_normalized = {model: weight / total_weight for model, weight in self.model_weights.items()}\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Zbieramy predykcje każdego modelu pomnożone przez ich wagi\n",
    "            weighted_preds = []\n",
    "            for model_name, model in self.models.items():\n",
    "                probs = torch.softmax(model(images), dim=1)\n",
    "                weight = model_weights_normalized.get(model_name, 0.0)  # Domyślna waga 0, jeśli model nie ma wagi\n",
    "                weighted_preds.append(probs * weight)\n",
    "\n",
    "            # Suma ważonych predykcji\n",
    "            ensemble_probs = torch.sum(torch.stack(weighted_preds), dim=0)\n",
    "\n",
    "            # Ostateczne predykcje\n",
    "            _, ensemble_preds = torch.max(ensemble_probs, 1)\n",
    "\n",
    "            all_ensemble_preds.extend(ensemble_preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Obliczenie F1-score dla ostatecznych predykcji\n",
    "        ensemble_f1 = f1_score(all_labels, all_ensemble_preds, average='macro')\n",
    "        print(f\"Weighted Ensemble F1 Score: {ensemble_f1:.4f}\")\n",
    "\n",
    "        return ensemble_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-5c1a4163.pth\" to C:\\Users\\dimpi/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_large-5c1a4163.pth\n",
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 26.9MB/s]\n",
      "c:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dimpi\\.cache\\huggingface\\hub\\models--timm--vit_tiny_patch16_224.augreg_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training mobilenet_v3_large\n",
      "mobilenet_v3_large - Current time: 54.0 s, epoch: 1/50, minibatch:    20/400, running loss: 0.045\n",
      "mobilenet_v3_large - Current time: 127.0 s, epoch: 1/50, minibatch:    40/400, running loss: 0.028\n",
      "mobilenet_v3_large - Current time: 197.0 s, epoch: 1/50, minibatch:    60/400, running loss: 0.029\n",
      "mobilenet_v3_large - Current time: 270.0 s, epoch: 1/50, minibatch:    80/400, running loss: 0.026\n",
      "mobilenet_v3_large - Current time: 349.0 s, epoch: 1/50, minibatch:   100/400, running loss: 0.024\n",
      "mobilenet_v3_large - Current time: 400.0 s, epoch: 1/50, minibatch:   120/400, running loss: 0.024\n",
      "mobilenet_v3_large - Current time: 460.0 s, epoch: 1/50, minibatch:   140/400, running loss: 0.023\n",
      "mobilenet_v3_large - Current time: 515.0 s, epoch: 1/50, minibatch:   160/400, running loss: 0.020\n",
      "mobilenet_v3_large - Current time: 564.0 s, epoch: 1/50, minibatch:   180/400, running loss: 0.020\n",
      "mobilenet_v3_large - Current time: 636.0 s, epoch: 1/50, minibatch:   200/400, running loss: 0.020\n",
      "mobilenet_v3_large - Current time: 687.0 s, epoch: 1/50, minibatch:   220/400, running loss: 0.015\n",
      "mobilenet_v3_large - Current time: 743.0 s, epoch: 1/50, minibatch:   240/400, running loss: 0.020\n",
      "mobilenet_v3_large - Current time: 811.0 s, epoch: 1/50, minibatch:   260/400, running loss: 0.017\n",
      "mobilenet_v3_large - Current time: 867.0 s, epoch: 1/50, minibatch:   280/400, running loss: 0.019\n",
      "mobilenet_v3_large - Current time: 903.0 s, epoch: 1/50, minibatch:   300/400, running loss: 0.017\n",
      "mobilenet_v3_large - Current time: 942.0 s, epoch: 1/50, minibatch:   320/400, running loss: 0.017\n",
      "mobilenet_v3_large - Current time: 977.0 s, epoch: 1/50, minibatch:   340/400, running loss: 0.017\n",
      "mobilenet_v3_large - Current time: 1023.0 s, epoch: 1/50, minibatch:   360/400, running loss: 0.017\n",
      "mobilenet_v3_large - Current time: 1059.0 s, epoch: 1/50, minibatch:   380/400, running loss: 0.016\n",
      "mobilenet_v3_large - Current time: 1092.0 s, epoch: 1/50, minibatch:   400/400, running loss: 0.017\n",
      "mobilenet_v3_large - Epoch 1: Train Loss = 0.5408, Val Loss = 0.6083, Val F1 = 0.8171\n",
      "mobilenet_v3_large - Current time: 36.0 s, epoch: 2/50, minibatch:    20/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 64.0 s, epoch: 2/50, minibatch:    40/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 92.0 s, epoch: 2/50, minibatch:    60/400, running loss: 0.017\n",
      "mobilenet_v3_large - Current time: 123.0 s, epoch: 2/50, minibatch:    80/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 153.0 s, epoch: 2/50, minibatch:   100/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 186.0 s, epoch: 2/50, minibatch:   120/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 219.0 s, epoch: 2/50, minibatch:   140/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 253.0 s, epoch: 2/50, minibatch:   160/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 289.0 s, epoch: 2/50, minibatch:   180/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 327.0 s, epoch: 2/50, minibatch:   200/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 370.0 s, epoch: 2/50, minibatch:   220/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 424.0 s, epoch: 2/50, minibatch:   240/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 465.0 s, epoch: 2/50, minibatch:   260/400, running loss: 0.013\n",
      "mobilenet_v3_large - Current time: 500.0 s, epoch: 2/50, minibatch:   280/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 836.0 s, epoch: 2/50, minibatch:   300/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 905.0 s, epoch: 2/50, minibatch:   320/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 942.0 s, epoch: 2/50, minibatch:   340/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 980.0 s, epoch: 2/50, minibatch:   360/400, running loss: 0.013\n",
      "mobilenet_v3_large - Current time: 1028.0 s, epoch: 2/50, minibatch:   380/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 1068.0 s, epoch: 2/50, minibatch:   400/400, running loss: 0.011\n",
      "mobilenet_v3_large - Epoch 2: Train Loss = 0.3277, Val Loss = 0.5324, Val F1 = 0.8320\n",
      "mobilenet_v3_large - Current time: 35.0 s, epoch: 3/50, minibatch:    20/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 80.0 s, epoch: 3/50, minibatch:    40/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 147.0 s, epoch: 3/50, minibatch:    60/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 188.0 s, epoch: 3/50, minibatch:    80/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 223.0 s, epoch: 3/50, minibatch:   100/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 262.0 s, epoch: 3/50, minibatch:   120/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 302.0 s, epoch: 3/50, minibatch:   140/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 341.0 s, epoch: 3/50, minibatch:   160/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 396.0 s, epoch: 3/50, minibatch:   180/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 441.0 s, epoch: 3/50, minibatch:   200/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 479.0 s, epoch: 3/50, minibatch:   220/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 551.0 s, epoch: 3/50, minibatch:   240/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 631.0 s, epoch: 3/50, minibatch:   260/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 680.0 s, epoch: 3/50, minibatch:   280/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 718.0 s, epoch: 3/50, minibatch:   300/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 752.0 s, epoch: 3/50, minibatch:   320/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 817.0 s, epoch: 3/50, minibatch:   340/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 867.0 s, epoch: 3/50, minibatch:   360/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 913.0 s, epoch: 3/50, minibatch:   380/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 962.0 s, epoch: 3/50, minibatch:   400/400, running loss: 0.012\n",
      "mobilenet_v3_large - Epoch 3: Train Loss = 0.2690, Val Loss = 0.4436, Val F1 = 0.8572\n",
      "mobilenet_v3_large - Current time: 68.0 s, epoch: 4/50, minibatch:    20/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 153.0 s, epoch: 4/50, minibatch:    40/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 237.0 s, epoch: 4/50, minibatch:    60/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 324.0 s, epoch: 4/50, minibatch:    80/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 370.0 s, epoch: 4/50, minibatch:   100/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 416.0 s, epoch: 4/50, minibatch:   120/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 462.0 s, epoch: 4/50, minibatch:   140/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 528.0 s, epoch: 4/50, minibatch:   160/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 592.0 s, epoch: 4/50, minibatch:   180/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 676.0 s, epoch: 4/50, minibatch:   200/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 723.0 s, epoch: 4/50, minibatch:   220/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 768.0 s, epoch: 4/50, minibatch:   240/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 813.0 s, epoch: 4/50, minibatch:   260/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 883.0 s, epoch: 4/50, minibatch:   280/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 967.0 s, epoch: 4/50, minibatch:   300/400, running loss: 0.013\n",
      "mobilenet_v3_large - Current time: 1053.0 s, epoch: 4/50, minibatch:   320/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 1136.0 s, epoch: 4/50, minibatch:   340/400, running loss: 0.012\n",
      "mobilenet_v3_large - Current time: 1216.0 s, epoch: 4/50, minibatch:   360/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 1299.0 s, epoch: 4/50, minibatch:   380/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 1378.0 s, epoch: 4/50, minibatch:   400/400, running loss: 0.009\n",
      "mobilenet_v3_large - Epoch 4: Train Loss = 0.2504, Val Loss = 0.3232, Val F1 = 0.8948\n",
      "mobilenet_v3_large - Current time: 81.0 s, epoch: 5/50, minibatch:    20/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 161.0 s, epoch: 5/50, minibatch:    40/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 243.0 s, epoch: 5/50, minibatch:    60/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 325.0 s, epoch: 5/50, minibatch:    80/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 406.0 s, epoch: 5/50, minibatch:   100/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 487.0 s, epoch: 5/50, minibatch:   120/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 568.0 s, epoch: 5/50, minibatch:   140/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 650.0 s, epoch: 5/50, minibatch:   160/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 732.0 s, epoch: 5/50, minibatch:   180/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 820.0 s, epoch: 5/50, minibatch:   200/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 895.0 s, epoch: 5/50, minibatch:   220/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 983.0 s, epoch: 5/50, minibatch:   240/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1069.0 s, epoch: 5/50, minibatch:   260/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 1148.0 s, epoch: 5/50, minibatch:   280/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1244.0 s, epoch: 5/50, minibatch:   300/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 1450.0 s, epoch: 5/50, minibatch:   320/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 1538.0 s, epoch: 5/50, minibatch:   340/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1609.0 s, epoch: 5/50, minibatch:   360/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1672.0 s, epoch: 5/50, minibatch:   380/400, running loss: 0.014\n",
      "mobilenet_v3_large - Current time: 1717.0 s, epoch: 5/50, minibatch:   400/400, running loss: 0.010\n",
      "mobilenet_v3_large - Epoch 5: Train Loss = 0.2072, Val Loss = 0.3396, Val F1 = 0.8927\n",
      "mobilenet_v3_large - Current time: 65.0 s, epoch: 6/50, minibatch:    20/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 115.0 s, epoch: 6/50, minibatch:    40/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 171.0 s, epoch: 6/50, minibatch:    60/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 228.0 s, epoch: 6/50, minibatch:    80/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 301.0 s, epoch: 6/50, minibatch:   100/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 416.0 s, epoch: 6/50, minibatch:   120/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 504.0 s, epoch: 6/50, minibatch:   140/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 578.0 s, epoch: 6/50, minibatch:   160/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 668.0 s, epoch: 6/50, minibatch:   180/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 985.0 s, epoch: 6/50, minibatch:   200/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1076.0 s, epoch: 6/50, minibatch:   220/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 1180.0 s, epoch: 6/50, minibatch:   240/400, running loss: 0.011\n",
      "mobilenet_v3_large - Current time: 1244.0 s, epoch: 6/50, minibatch:   260/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 1298.0 s, epoch: 6/50, minibatch:   280/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1337.0 s, epoch: 6/50, minibatch:   300/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1375.0 s, epoch: 6/50, minibatch:   320/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 1414.0 s, epoch: 6/50, minibatch:   340/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 1465.0 s, epoch: 6/50, minibatch:   360/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1525.0 s, epoch: 6/50, minibatch:   380/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1617.0 s, epoch: 6/50, minibatch:   400/400, running loss: 0.011\n",
      "mobilenet_v3_large - Epoch 6: Train Loss = 0.1992, Val Loss = 0.3051, Val F1 = 0.8998\n",
      "mobilenet_v3_large - Current time: 79.0 s, epoch: 7/50, minibatch:    20/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 171.0 s, epoch: 7/50, minibatch:    40/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 552.0 s, epoch: 7/50, minibatch:    60/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1121.0 s, epoch: 7/50, minibatch:    80/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1169.0 s, epoch: 7/50, minibatch:   100/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1207.0 s, epoch: 7/50, minibatch:   120/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1283.0 s, epoch: 7/50, minibatch:   140/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 1340.0 s, epoch: 7/50, minibatch:   160/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 1405.0 s, epoch: 7/50, minibatch:   180/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 1466.0 s, epoch: 7/50, minibatch:   200/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1526.0 s, epoch: 7/50, minibatch:   220/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 1591.0 s, epoch: 7/50, minibatch:   240/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1632.0 s, epoch: 7/50, minibatch:   260/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 1710.0 s, epoch: 7/50, minibatch:   280/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 1790.0 s, epoch: 7/50, minibatch:   300/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 1875.0 s, epoch: 7/50, minibatch:   320/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1931.0 s, epoch: 7/50, minibatch:   340/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 2011.0 s, epoch: 7/50, minibatch:   360/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 2088.0 s, epoch: 7/50, minibatch:   380/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 2128.0 s, epoch: 7/50, minibatch:   400/400, running loss: 0.005\n",
      "mobilenet_v3_large - Epoch 7: Train Loss = 0.1935, Val Loss = 0.3021, Val F1 = 0.8977\n",
      "mobilenet_v3_large - Current time: 44.0 s, epoch: 8/50, minibatch:    20/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 82.0 s, epoch: 8/50, minibatch:    40/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 121.0 s, epoch: 8/50, minibatch:    60/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 175.0 s, epoch: 8/50, minibatch:    80/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 213.0 s, epoch: 8/50, minibatch:   100/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 266.0 s, epoch: 8/50, minibatch:   120/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 308.0 s, epoch: 8/50, minibatch:   140/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 345.0 s, epoch: 8/50, minibatch:   160/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 385.0 s, epoch: 8/50, minibatch:   180/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 420.0 s, epoch: 8/50, minibatch:   200/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 463.0 s, epoch: 8/50, minibatch:   220/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 502.0 s, epoch: 8/50, minibatch:   240/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 544.0 s, epoch: 8/50, minibatch:   260/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 612.0 s, epoch: 8/50, minibatch:   280/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 654.0 s, epoch: 8/50, minibatch:   300/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 695.0 s, epoch: 8/50, minibatch:   320/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 727.0 s, epoch: 8/50, minibatch:   340/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 760.0 s, epoch: 8/50, minibatch:   360/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 847.0 s, epoch: 8/50, minibatch:   380/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 927.0 s, epoch: 8/50, minibatch:   400/400, running loss: 0.008\n",
      "mobilenet_v3_large - Epoch 8: Train Loss = 0.1709, Val Loss = 0.4338, Val F1 = 0.8668\n",
      "mobilenet_v3_large - Current time: 49.0 s, epoch: 9/50, minibatch:    20/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 81.0 s, epoch: 9/50, minibatch:    40/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 118.0 s, epoch: 9/50, minibatch:    60/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 156.0 s, epoch: 9/50, minibatch:    80/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 197.0 s, epoch: 9/50, minibatch:   100/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 248.0 s, epoch: 9/50, minibatch:   120/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 303.0 s, epoch: 9/50, minibatch:   140/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 355.0 s, epoch: 9/50, minibatch:   160/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 396.0 s, epoch: 9/50, minibatch:   180/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 435.0 s, epoch: 9/50, minibatch:   200/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 478.0 s, epoch: 9/50, minibatch:   220/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 528.0 s, epoch: 9/50, minibatch:   240/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 607.0 s, epoch: 9/50, minibatch:   260/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 644.0 s, epoch: 9/50, minibatch:   280/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 681.0 s, epoch: 9/50, minibatch:   300/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 718.0 s, epoch: 9/50, minibatch:   320/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 778.0 s, epoch: 9/50, minibatch:   340/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 934.0 s, epoch: 9/50, minibatch:   360/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1107.0 s, epoch: 9/50, minibatch:   380/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1195.0 s, epoch: 9/50, minibatch:   400/400, running loss: 0.006\n",
      "mobilenet_v3_large - Epoch 9: Train Loss = 0.1744, Val Loss = 0.2907, Val F1 = 0.9074\n",
      "mobilenet_v3_large - Current time: 44.0 s, epoch: 10/50, minibatch:    20/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 147.0 s, epoch: 10/50, minibatch:    40/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 351.0 s, epoch: 10/50, minibatch:    60/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 550.0 s, epoch: 10/50, minibatch:    80/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 775.0 s, epoch: 10/50, minibatch:   100/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 816.0 s, epoch: 10/50, minibatch:   120/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 856.0 s, epoch: 10/50, minibatch:   140/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 901.0 s, epoch: 10/50, minibatch:   160/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 937.0 s, epoch: 10/50, minibatch:   180/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 971.0 s, epoch: 10/50, minibatch:   200/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1067.0 s, epoch: 10/50, minibatch:   220/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 1273.0 s, epoch: 10/50, minibatch:   240/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1419.0 s, epoch: 10/50, minibatch:   260/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1444.0 s, epoch: 10/50, minibatch:   280/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1472.0 s, epoch: 10/50, minibatch:   300/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1500.0 s, epoch: 10/50, minibatch:   320/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1526.0 s, epoch: 10/50, minibatch:   340/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1554.0 s, epoch: 10/50, minibatch:   360/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 1587.0 s, epoch: 10/50, minibatch:   380/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1621.0 s, epoch: 10/50, minibatch:   400/400, running loss: 0.006\n",
      "mobilenet_v3_large - Epoch 10: Train Loss = 0.1702, Val Loss = 0.3667, Val F1 = 0.8860\n",
      "mobilenet_v3_large - Current time: 33.0 s, epoch: 11/50, minibatch:    20/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 63.0 s, epoch: 11/50, minibatch:    40/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 103.0 s, epoch: 11/50, minibatch:    60/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 145.0 s, epoch: 11/50, minibatch:    80/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 182.0 s, epoch: 11/50, minibatch:   100/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 216.0 s, epoch: 11/50, minibatch:   120/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 250.0 s, epoch: 11/50, minibatch:   140/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 283.0 s, epoch: 11/50, minibatch:   160/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 319.0 s, epoch: 11/50, minibatch:   180/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 351.0 s, epoch: 11/50, minibatch:   200/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 381.0 s, epoch: 11/50, minibatch:   220/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 410.0 s, epoch: 11/50, minibatch:   240/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 453.0 s, epoch: 11/50, minibatch:   260/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 493.0 s, epoch: 11/50, minibatch:   280/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 524.0 s, epoch: 11/50, minibatch:   300/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 555.0 s, epoch: 11/50, minibatch:   320/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 584.0 s, epoch: 11/50, minibatch:   340/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 615.0 s, epoch: 11/50, minibatch:   360/400, running loss: 0.010\n",
      "mobilenet_v3_large - Current time: 691.0 s, epoch: 11/50, minibatch:   380/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 778.0 s, epoch: 11/50, minibatch:   400/400, running loss: 0.007\n",
      "mobilenet_v3_large - Epoch 11: Train Loss = 0.1556, Val Loss = 0.2213, Val F1 = 0.9229\n",
      "mobilenet_v3_large - Current time: 86.0 s, epoch: 12/50, minibatch:    20/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 169.0 s, epoch: 12/50, minibatch:    40/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 254.0 s, epoch: 12/50, minibatch:    60/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 341.0 s, epoch: 12/50, minibatch:    80/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 427.0 s, epoch: 12/50, minibatch:   100/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 501.0 s, epoch: 12/50, minibatch:   120/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 560.0 s, epoch: 12/50, minibatch:   140/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 609.0 s, epoch: 12/50, minibatch:   160/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 660.0 s, epoch: 12/50, minibatch:   180/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 714.0 s, epoch: 12/50, minibatch:   200/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 779.0 s, epoch: 12/50, minibatch:   220/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 820.0 s, epoch: 12/50, minibatch:   240/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 862.0 s, epoch: 12/50, minibatch:   260/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 909.0 s, epoch: 12/50, minibatch:   280/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 946.0 s, epoch: 12/50, minibatch:   300/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 984.0 s, epoch: 12/50, minibatch:   320/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 1026.0 s, epoch: 12/50, minibatch:   340/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 1066.0 s, epoch: 12/50, minibatch:   360/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 1107.0 s, epoch: 12/50, minibatch:   380/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 1145.0 s, epoch: 12/50, minibatch:   400/400, running loss: 0.007\n",
      "mobilenet_v3_large - Epoch 12: Train Loss = 0.1518, Val Loss = 0.2558, Val F1 = 0.9162\n",
      "mobilenet_v3_large - Current time: 42.0 s, epoch: 13/50, minibatch:    20/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 77.0 s, epoch: 13/50, minibatch:    40/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 110.0 s, epoch: 13/50, minibatch:    60/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 142.0 s, epoch: 13/50, minibatch:    80/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 180.0 s, epoch: 13/50, minibatch:   100/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 230.0 s, epoch: 13/50, minibatch:   120/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 287.0 s, epoch: 13/50, minibatch:   140/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 356.0 s, epoch: 13/50, minibatch:   160/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 417.0 s, epoch: 13/50, minibatch:   180/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 475.0 s, epoch: 13/50, minibatch:   200/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 512.0 s, epoch: 13/50, minibatch:   220/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 551.0 s, epoch: 13/50, minibatch:   240/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 582.0 s, epoch: 13/50, minibatch:   260/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 615.0 s, epoch: 13/50, minibatch:   280/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 648.0 s, epoch: 13/50, minibatch:   300/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 690.0 s, epoch: 13/50, minibatch:   320/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 738.0 s, epoch: 13/50, minibatch:   340/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 776.0 s, epoch: 13/50, minibatch:   360/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 812.0 s, epoch: 13/50, minibatch:   380/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 851.0 s, epoch: 13/50, minibatch:   400/400, running loss: 0.007\n",
      "mobilenet_v3_large - Epoch 13: Train Loss = 0.1468, Val Loss = 0.2912, Val F1 = 0.9089\n",
      "mobilenet_v3_large - Current time: 70.0 s, epoch: 14/50, minibatch:    20/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 154.0 s, epoch: 14/50, minibatch:    40/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 194.0 s, epoch: 14/50, minibatch:    60/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 236.0 s, epoch: 14/50, minibatch:    80/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 332.0 s, epoch: 14/50, minibatch:   100/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 428.0 s, epoch: 14/50, minibatch:   120/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 535.0 s, epoch: 14/50, minibatch:   140/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 602.0 s, epoch: 14/50, minibatch:   160/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 694.0 s, epoch: 14/50, minibatch:   180/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 739.0 s, epoch: 14/50, minibatch:   200/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 773.0 s, epoch: 14/50, minibatch:   220/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 807.0 s, epoch: 14/50, minibatch:   240/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 840.0 s, epoch: 14/50, minibatch:   260/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 873.0 s, epoch: 14/50, minibatch:   280/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 909.0 s, epoch: 14/50, minibatch:   300/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 942.0 s, epoch: 14/50, minibatch:   320/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 975.0 s, epoch: 14/50, minibatch:   340/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 1008.0 s, epoch: 14/50, minibatch:   360/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 1041.0 s, epoch: 14/50, minibatch:   380/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 1074.0 s, epoch: 14/50, minibatch:   400/400, running loss: 0.006\n",
      "mobilenet_v3_large - Epoch 14: Train Loss = 0.1415, Val Loss = 0.2467, Val F1 = 0.9184\n",
      "mobilenet_v3_large - Current time: 32.0 s, epoch: 15/50, minibatch:    20/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 66.0 s, epoch: 15/50, minibatch:    40/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 99.0 s, epoch: 15/50, minibatch:    60/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 131.0 s, epoch: 15/50, minibatch:    80/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 164.0 s, epoch: 15/50, minibatch:   100/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 196.0 s, epoch: 15/50, minibatch:   120/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 229.0 s, epoch: 15/50, minibatch:   140/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 262.0 s, epoch: 15/50, minibatch:   160/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 295.0 s, epoch: 15/50, minibatch:   180/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 327.0 s, epoch: 15/50, minibatch:   200/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 360.0 s, epoch: 15/50, minibatch:   220/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 392.0 s, epoch: 15/50, minibatch:   240/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 425.0 s, epoch: 15/50, minibatch:   260/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 457.0 s, epoch: 15/50, minibatch:   280/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 489.0 s, epoch: 15/50, minibatch:   300/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 522.0 s, epoch: 15/50, minibatch:   320/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 555.0 s, epoch: 15/50, minibatch:   340/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 588.0 s, epoch: 15/50, minibatch:   360/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 621.0 s, epoch: 15/50, minibatch:   380/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 652.0 s, epoch: 15/50, minibatch:   400/400, running loss: 0.007\n",
      "mobilenet_v3_large - Epoch 15: Train Loss = 0.1519, Val Loss = 0.2599, Val F1 = 0.9209\n",
      "mobilenet_v3_large - Current time: 33.0 s, epoch: 16/50, minibatch:    20/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 65.0 s, epoch: 16/50, minibatch:    40/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 98.0 s, epoch: 16/50, minibatch:    60/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 131.0 s, epoch: 16/50, minibatch:    80/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 164.0 s, epoch: 16/50, minibatch:   100/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 197.0 s, epoch: 16/50, minibatch:   120/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 230.0 s, epoch: 16/50, minibatch:   140/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 262.0 s, epoch: 16/50, minibatch:   160/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 295.0 s, epoch: 16/50, minibatch:   180/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 327.0 s, epoch: 16/50, minibatch:   200/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 360.0 s, epoch: 16/50, minibatch:   220/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 392.0 s, epoch: 16/50, minibatch:   240/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 424.0 s, epoch: 16/50, minibatch:   260/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 456.0 s, epoch: 16/50, minibatch:   280/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 489.0 s, epoch: 16/50, minibatch:   300/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 522.0 s, epoch: 16/50, minibatch:   320/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 555.0 s, epoch: 16/50, minibatch:   340/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 587.0 s, epoch: 16/50, minibatch:   360/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 620.0 s, epoch: 16/50, minibatch:   380/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 653.0 s, epoch: 16/50, minibatch:   400/400, running loss: 0.006\n",
      "mobilenet_v3_large - Epoch 16: Train Loss = 0.1341, Val Loss = 0.2552, Val F1 = 0.9204\n",
      "mobilenet_v3_large - Current time: 33.0 s, epoch: 17/50, minibatch:    20/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 65.0 s, epoch: 17/50, minibatch:    40/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 98.0 s, epoch: 17/50, minibatch:    60/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 131.0 s, epoch: 17/50, minibatch:    80/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 164.0 s, epoch: 17/50, minibatch:   100/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 196.0 s, epoch: 17/50, minibatch:   120/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 229.0 s, epoch: 17/50, minibatch:   140/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 262.0 s, epoch: 17/50, minibatch:   160/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 294.0 s, epoch: 17/50, minibatch:   180/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 327.0 s, epoch: 17/50, minibatch:   200/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 360.0 s, epoch: 17/50, minibatch:   220/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 393.0 s, epoch: 17/50, minibatch:   240/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 426.0 s, epoch: 17/50, minibatch:   260/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 459.0 s, epoch: 17/50, minibatch:   280/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 492.0 s, epoch: 17/50, minibatch:   300/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 525.0 s, epoch: 17/50, minibatch:   320/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 557.0 s, epoch: 17/50, minibatch:   340/400, running loss: 0.009\n",
      "mobilenet_v3_large - Current time: 590.0 s, epoch: 17/50, minibatch:   360/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 622.0 s, epoch: 17/50, minibatch:   380/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 654.0 s, epoch: 17/50, minibatch:   400/400, running loss: 0.005\n",
      "mobilenet_v3_large - Epoch 17: Train Loss = 0.1475, Val Loss = 0.2602, Val F1 = 0.9173\n",
      "mobilenet_v3_large - Current time: 33.0 s, epoch: 18/50, minibatch:    20/400, running loss: 0.003\n",
      "mobilenet_v3_large - Current time: 66.0 s, epoch: 18/50, minibatch:    40/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 98.0 s, epoch: 18/50, minibatch:    60/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 130.0 s, epoch: 18/50, minibatch:    80/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 162.0 s, epoch: 18/50, minibatch:   100/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 195.0 s, epoch: 18/50, minibatch:   120/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 228.0 s, epoch: 18/50, minibatch:   140/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 261.0 s, epoch: 18/50, minibatch:   160/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 294.0 s, epoch: 18/50, minibatch:   180/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 327.0 s, epoch: 18/50, minibatch:   200/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 360.0 s, epoch: 18/50, minibatch:   220/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 392.0 s, epoch: 18/50, minibatch:   240/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 424.0 s, epoch: 18/50, minibatch:   260/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 457.0 s, epoch: 18/50, minibatch:   280/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 490.0 s, epoch: 18/50, minibatch:   300/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 523.0 s, epoch: 18/50, minibatch:   320/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 556.0 s, epoch: 18/50, minibatch:   340/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 589.0 s, epoch: 18/50, minibatch:   360/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 622.0 s, epoch: 18/50, minibatch:   380/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 654.0 s, epoch: 18/50, minibatch:   400/400, running loss: 0.006\n",
      "mobilenet_v3_large - Epoch 18: Train Loss = 0.1346, Val Loss = 0.2943, Val F1 = 0.9080\n",
      "mobilenet_v3_large - Current time: 33.0 s, epoch: 19/50, minibatch:    20/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 65.0 s, epoch: 19/50, minibatch:    40/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 97.0 s, epoch: 19/50, minibatch:    60/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 130.0 s, epoch: 19/50, minibatch:    80/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 163.0 s, epoch: 19/50, minibatch:   100/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 195.0 s, epoch: 19/50, minibatch:   120/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 228.0 s, epoch: 19/50, minibatch:   140/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 260.0 s, epoch: 19/50, minibatch:   160/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 292.0 s, epoch: 19/50, minibatch:   180/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 325.0 s, epoch: 19/50, minibatch:   200/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 357.0 s, epoch: 19/50, minibatch:   220/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 390.0 s, epoch: 19/50, minibatch:   240/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 423.0 s, epoch: 19/50, minibatch:   260/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 455.0 s, epoch: 19/50, minibatch:   280/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 488.0 s, epoch: 19/50, minibatch:   300/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 521.0 s, epoch: 19/50, minibatch:   320/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 553.0 s, epoch: 19/50, minibatch:   340/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 586.0 s, epoch: 19/50, minibatch:   360/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 619.0 s, epoch: 19/50, minibatch:   380/400, running loss: 0.008\n",
      "mobilenet_v3_large - Current time: 652.0 s, epoch: 19/50, minibatch:   400/400, running loss: 0.006\n",
      "mobilenet_v3_large - Epoch 19: Train Loss = 0.1243, Val Loss = 0.2372, Val F1 = 0.9202\n",
      "mobilenet_v3_large - Current time: 33.0 s, epoch: 20/50, minibatch:    20/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 65.0 s, epoch: 20/50, minibatch:    40/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 98.0 s, epoch: 20/50, minibatch:    60/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 131.0 s, epoch: 20/50, minibatch:    80/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 164.0 s, epoch: 20/50, minibatch:   100/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 197.0 s, epoch: 20/50, minibatch:   120/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 230.0 s, epoch: 20/50, minibatch:   140/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 263.0 s, epoch: 20/50, minibatch:   160/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 295.0 s, epoch: 20/50, minibatch:   180/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 327.0 s, epoch: 20/50, minibatch:   200/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 360.0 s, epoch: 20/50, minibatch:   220/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 392.0 s, epoch: 20/50, minibatch:   240/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 424.0 s, epoch: 20/50, minibatch:   260/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 456.0 s, epoch: 20/50, minibatch:   280/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 489.0 s, epoch: 20/50, minibatch:   300/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 523.0 s, epoch: 20/50, minibatch:   320/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 555.0 s, epoch: 20/50, minibatch:   340/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 588.0 s, epoch: 20/50, minibatch:   360/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 621.0 s, epoch: 20/50, minibatch:   380/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 653.0 s, epoch: 20/50, minibatch:   400/400, running loss: 0.005\n",
      "mobilenet_v3_large - Epoch 20: Train Loss = 0.1350, Val Loss = 0.2725, Val F1 = 0.9185\n",
      "mobilenet_v3_large - Current time: 33.0 s, epoch: 21/50, minibatch:    20/400, running loss: 0.003\n",
      "mobilenet_v3_large - Current time: 66.0 s, epoch: 21/50, minibatch:    40/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 99.0 s, epoch: 21/50, minibatch:    60/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 132.0 s, epoch: 21/50, minibatch:    80/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 165.0 s, epoch: 21/50, minibatch:   100/400, running loss: 0.003\n",
      "mobilenet_v3_large - Current time: 197.0 s, epoch: 21/50, minibatch:   120/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 231.0 s, epoch: 21/50, minibatch:   140/400, running loss: 0.003\n",
      "mobilenet_v3_large - Current time: 263.0 s, epoch: 21/50, minibatch:   160/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 296.0 s, epoch: 21/50, minibatch:   180/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 328.0 s, epoch: 21/50, minibatch:   200/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 362.0 s, epoch: 21/50, minibatch:   220/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 394.0 s, epoch: 21/50, minibatch:   240/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 427.0 s, epoch: 21/50, minibatch:   260/400, running loss: 0.006\n",
      "mobilenet_v3_large - Current time: 459.0 s, epoch: 21/50, minibatch:   280/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 491.0 s, epoch: 21/50, minibatch:   300/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 524.0 s, epoch: 21/50, minibatch:   320/400, running loss: 0.004\n",
      "mobilenet_v3_large - Current time: 557.0 s, epoch: 21/50, minibatch:   340/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 590.0 s, epoch: 21/50, minibatch:   360/400, running loss: 0.005\n",
      "mobilenet_v3_large - Current time: 622.0 s, epoch: 21/50, minibatch:   380/400, running loss: 0.007\n",
      "mobilenet_v3_large - Current time: 655.0 s, epoch: 21/50, minibatch:   400/400, running loss: 0.006\n",
      "mobilenet_v3_large - Epoch 21: Train Loss = 0.1230, Val Loss = 0.2939, Val F1 = 0.9047\n",
      "mobilenet_v3_large - Early stopping triggered at epoch 21, best f1 score 0.9162076142682668\n",
      "\n",
      "Training vit_tiny\n",
      "vit_tiny - Current time: 90.0 s, epoch: 1/50, minibatch:    20/400, running loss: 0.093\n",
      "vit_tiny - Current time: 179.0 s, epoch: 1/50, minibatch:    40/400, running loss: 0.084\n",
      "vit_tiny - Current time: 268.0 s, epoch: 1/50, minibatch:    60/400, running loss: 0.083\n",
      "vit_tiny - Current time: 357.0 s, epoch: 1/50, minibatch:    80/400, running loss: 0.081\n",
      "vit_tiny - Current time: 446.0 s, epoch: 1/50, minibatch:   100/400, running loss: 0.082\n",
      "vit_tiny - Current time: 535.0 s, epoch: 1/50, minibatch:   120/400, running loss: 0.081\n",
      "vit_tiny - Current time: 623.0 s, epoch: 1/50, minibatch:   140/400, running loss: 0.079\n",
      "vit_tiny - Current time: 712.0 s, epoch: 1/50, minibatch:   160/400, running loss: 0.078\n",
      "vit_tiny - Current time: 800.0 s, epoch: 1/50, minibatch:   180/400, running loss: 0.080\n",
      "vit_tiny - Current time: 890.0 s, epoch: 1/50, minibatch:   200/400, running loss: 0.075\n",
      "vit_tiny - Current time: 979.0 s, epoch: 1/50, minibatch:   220/400, running loss: 0.076\n",
      "vit_tiny - Current time: 1067.0 s, epoch: 1/50, minibatch:   240/400, running loss: 0.073\n",
      "vit_tiny - Current time: 1156.0 s, epoch: 1/50, minibatch:   260/400, running loss: 0.072\n",
      "vit_tiny - Current time: 1245.0 s, epoch: 1/50, minibatch:   280/400, running loss: 0.076\n",
      "vit_tiny - Current time: 1334.0 s, epoch: 1/50, minibatch:   300/400, running loss: 0.074\n",
      "vit_tiny - Current time: 1424.0 s, epoch: 1/50, minibatch:   320/400, running loss: 0.069\n",
      "vit_tiny - Current time: 1513.0 s, epoch: 1/50, minibatch:   340/400, running loss: 0.070\n",
      "vit_tiny - Current time: 1603.0 s, epoch: 1/50, minibatch:   360/400, running loss: 0.070\n",
      "vit_tiny - Current time: 1692.0 s, epoch: 1/50, minibatch:   380/400, running loss: 0.066\n",
      "vit_tiny - Current time: 1781.0 s, epoch: 1/50, minibatch:   400/400, running loss: 0.067\n",
      "vit_tiny - Epoch 1: Train Loss = 1.9097, Val Loss = 1.9093, Val F1 = 0.2871\n",
      "vit_tiny - Current time: 90.0 s, epoch: 2/50, minibatch:    20/400, running loss: 0.069\n",
      "vit_tiny - Current time: 181.0 s, epoch: 2/50, minibatch:    40/400, running loss: 0.067\n",
      "vit_tiny - Current time: 271.0 s, epoch: 2/50, minibatch:    60/400, running loss: 0.065\n",
      "vit_tiny - Current time: 361.0 s, epoch: 2/50, minibatch:    80/400, running loss: 0.068\n",
      "vit_tiny - Current time: 452.0 s, epoch: 2/50, minibatch:   100/400, running loss: 0.064\n",
      "vit_tiny - Current time: 542.0 s, epoch: 2/50, minibatch:   120/400, running loss: 0.065\n",
      "vit_tiny - Current time: 633.0 s, epoch: 2/50, minibatch:   140/400, running loss: 0.062\n",
      "vit_tiny - Current time: 723.0 s, epoch: 2/50, minibatch:   160/400, running loss: 0.063\n",
      "vit_tiny - Current time: 814.0 s, epoch: 2/50, minibatch:   180/400, running loss: 0.063\n",
      "vit_tiny - Current time: 904.0 s, epoch: 2/50, minibatch:   200/400, running loss: 0.063\n",
      "vit_tiny - Current time: 995.0 s, epoch: 2/50, minibatch:   220/400, running loss: 0.062\n",
      "vit_tiny - Current time: 1085.0 s, epoch: 2/50, minibatch:   240/400, running loss: 0.059\n",
      "vit_tiny - Current time: 1176.0 s, epoch: 2/50, minibatch:   260/400, running loss: 0.062\n",
      "vit_tiny - Current time: 1266.0 s, epoch: 2/50, minibatch:   280/400, running loss: 0.061\n",
      "vit_tiny - Current time: 1357.0 s, epoch: 2/50, minibatch:   300/400, running loss: 0.059\n",
      "vit_tiny - Current time: 1448.0 s, epoch: 2/50, minibatch:   320/400, running loss: 0.062\n",
      "vit_tiny - Current time: 1538.0 s, epoch: 2/50, minibatch:   340/400, running loss: 0.056\n",
      "vit_tiny - Current time: 1628.0 s, epoch: 2/50, minibatch:   360/400, running loss: 0.059\n",
      "vit_tiny - Current time: 1719.0 s, epoch: 2/50, minibatch:   380/400, running loss: 0.059\n",
      "vit_tiny - Current time: 1810.0 s, epoch: 2/50, minibatch:   400/400, running loss: 0.063\n",
      "vit_tiny - Epoch 2: Train Loss = 1.5631, Val Loss = 1.5739, Val F1 = 0.3957\n",
      "vit_tiny - Current time: 91.0 s, epoch: 3/50, minibatch:    20/400, running loss: 0.058\n",
      "vit_tiny - Current time: 182.0 s, epoch: 3/50, minibatch:    40/400, running loss: 0.058\n",
      "vit_tiny - Current time: 273.0 s, epoch: 3/50, minibatch:    60/400, running loss: 0.055\n",
      "vit_tiny - Current time: 363.0 s, epoch: 3/50, minibatch:    80/400, running loss: 0.057\n",
      "vit_tiny - Current time: 453.0 s, epoch: 3/50, minibatch:   100/400, running loss: 0.058\n",
      "vit_tiny - Current time: 544.0 s, epoch: 3/50, minibatch:   120/400, running loss: 0.056\n",
      "vit_tiny - Current time: 634.0 s, epoch: 3/50, minibatch:   140/400, running loss: 0.057\n",
      "vit_tiny - Current time: 725.0 s, epoch: 3/50, minibatch:   160/400, running loss: 0.059\n",
      "vit_tiny - Current time: 815.0 s, epoch: 3/50, minibatch:   180/400, running loss: 0.059\n",
      "vit_tiny - Current time: 905.0 s, epoch: 3/50, minibatch:   200/400, running loss: 0.054\n",
      "vit_tiny - Current time: 996.0 s, epoch: 3/50, minibatch:   220/400, running loss: 0.055\n",
      "vit_tiny - Current time: 1086.0 s, epoch: 3/50, minibatch:   240/400, running loss: 0.057\n",
      "vit_tiny - Current time: 1177.0 s, epoch: 3/50, minibatch:   260/400, running loss: 0.054\n",
      "vit_tiny - Current time: 1268.0 s, epoch: 3/50, minibatch:   280/400, running loss: 0.053\n",
      "vit_tiny - Current time: 1359.0 s, epoch: 3/50, minibatch:   300/400, running loss: 0.052\n",
      "vit_tiny - Current time: 1449.0 s, epoch: 3/50, minibatch:   320/400, running loss: 0.058\n",
      "vit_tiny - Current time: 1539.0 s, epoch: 3/50, minibatch:   340/400, running loss: 0.054\n",
      "vit_tiny - Current time: 1629.0 s, epoch: 3/50, minibatch:   360/400, running loss: 0.050\n",
      "vit_tiny - Current time: 1720.0 s, epoch: 3/50, minibatch:   380/400, running loss: 0.056\n",
      "vit_tiny - Current time: 1811.0 s, epoch: 3/50, minibatch:   400/400, running loss: 0.054\n",
      "vit_tiny - Epoch 3: Train Loss = 1.3925, Val Loss = 1.3266, Val F1 = 0.5007\n",
      "vit_tiny - Current time: 91.0 s, epoch: 4/50, minibatch:    20/400, running loss: 0.053\n",
      "vit_tiny - Current time: 182.0 s, epoch: 4/50, minibatch:    40/400, running loss: 0.051\n",
      "vit_tiny - Current time: 273.0 s, epoch: 4/50, minibatch:    60/400, running loss: 0.053\n",
      "vit_tiny - Current time: 363.0 s, epoch: 4/50, minibatch:    80/400, running loss: 0.051\n",
      "vit_tiny - Current time: 455.0 s, epoch: 4/50, minibatch:   100/400, running loss: 0.053\n",
      "vit_tiny - Current time: 546.0 s, epoch: 4/50, minibatch:   120/400, running loss: 0.051\n",
      "vit_tiny - Current time: 636.0 s, epoch: 4/50, minibatch:   140/400, running loss: 0.057\n",
      "vit_tiny - Current time: 726.0 s, epoch: 4/50, minibatch:   160/400, running loss: 0.051\n",
      "vit_tiny - Current time: 816.0 s, epoch: 4/50, minibatch:   180/400, running loss: 0.054\n",
      "vit_tiny - Current time: 907.0 s, epoch: 4/50, minibatch:   200/400, running loss: 0.051\n",
      "vit_tiny - Current time: 998.0 s, epoch: 4/50, minibatch:   220/400, running loss: 0.049\n",
      "vit_tiny - Current time: 1089.0 s, epoch: 4/50, minibatch:   240/400, running loss: 0.053\n",
      "vit_tiny - Current time: 1180.0 s, epoch: 4/50, minibatch:   260/400, running loss: 0.047\n",
      "vit_tiny - Current time: 1269.0 s, epoch: 4/50, minibatch:   280/400, running loss: 0.051\n",
      "vit_tiny - Current time: 1360.0 s, epoch: 4/50, minibatch:   300/400, running loss: 0.054\n",
      "vit_tiny - Current time: 1451.0 s, epoch: 4/50, minibatch:   320/400, running loss: 0.046\n",
      "vit_tiny - Current time: 1542.0 s, epoch: 4/50, minibatch:   340/400, running loss: 0.050\n",
      "vit_tiny - Current time: 1632.0 s, epoch: 4/50, minibatch:   360/400, running loss: 0.049\n",
      "vit_tiny - Current time: 1723.0 s, epoch: 4/50, minibatch:   380/400, running loss: 0.050\n",
      "vit_tiny - Current time: 1813.0 s, epoch: 4/50, minibatch:   400/400, running loss: 0.051\n",
      "vit_tiny - Epoch 4: Train Loss = 1.2801, Val Loss = 1.2690, Val F1 = 0.5254\n",
      "vit_tiny - Current time: 90.0 s, epoch: 5/50, minibatch:    20/400, running loss: 0.047\n",
      "vit_tiny - Current time: 181.0 s, epoch: 5/50, minibatch:    40/400, running loss: 0.045\n",
      "vit_tiny - Current time: 271.0 s, epoch: 5/50, minibatch:    60/400, running loss: 0.050\n",
      "vit_tiny - Current time: 362.0 s, epoch: 5/50, minibatch:    80/400, running loss: 0.048\n",
      "vit_tiny - Current time: 453.0 s, epoch: 5/50, minibatch:   100/400, running loss: 0.049\n",
      "vit_tiny - Current time: 543.0 s, epoch: 5/50, minibatch:   120/400, running loss: 0.049\n",
      "vit_tiny - Current time: 634.0 s, epoch: 5/50, minibatch:   140/400, running loss: 0.049\n",
      "vit_tiny - Current time: 724.0 s, epoch: 5/50, minibatch:   160/400, running loss: 0.051\n",
      "vit_tiny - Current time: 814.0 s, epoch: 5/50, minibatch:   180/400, running loss: 0.049\n",
      "vit_tiny - Current time: 905.0 s, epoch: 5/50, minibatch:   200/400, running loss: 0.046\n",
      "vit_tiny - Current time: 996.0 s, epoch: 5/50, minibatch:   220/400, running loss: 0.050\n",
      "vit_tiny - Current time: 1087.0 s, epoch: 5/50, minibatch:   240/400, running loss: 0.050\n",
      "vit_tiny - Current time: 1177.0 s, epoch: 5/50, minibatch:   260/400, running loss: 0.047\n",
      "vit_tiny - Current time: 1268.0 s, epoch: 5/50, minibatch:   280/400, running loss: 0.045\n",
      "vit_tiny - Current time: 1359.0 s, epoch: 5/50, minibatch:   300/400, running loss: 0.046\n",
      "vit_tiny - Current time: 1450.0 s, epoch: 5/50, minibatch:   320/400, running loss: 0.050\n",
      "vit_tiny - Current time: 1539.0 s, epoch: 5/50, minibatch:   340/400, running loss: 0.048\n",
      "vit_tiny - Current time: 1630.0 s, epoch: 5/50, minibatch:   360/400, running loss: 0.044\n",
      "vit_tiny - Current time: 1721.0 s, epoch: 5/50, minibatch:   380/400, running loss: 0.048\n",
      "vit_tiny - Current time: 1811.0 s, epoch: 5/50, minibatch:   400/400, running loss: 0.049\n",
      "vit_tiny - Epoch 5: Train Loss = 1.1979, Val Loss = 1.1681, Val F1 = 0.5609\n",
      "vit_tiny - Current time: 91.0 s, epoch: 6/50, minibatch:    20/400, running loss: 0.041\n",
      "vit_tiny - Current time: 183.0 s, epoch: 6/50, minibatch:    40/400, running loss: 0.045\n",
      "vit_tiny - Current time: 272.0 s, epoch: 6/50, minibatch:    60/400, running loss: 0.048\n",
      "vit_tiny - Current time: 363.0 s, epoch: 6/50, minibatch:    80/400, running loss: 0.043\n",
      "vit_tiny - Current time: 454.0 s, epoch: 6/50, minibatch:   100/400, running loss: 0.044\n",
      "vit_tiny - Current time: 544.0 s, epoch: 6/50, minibatch:   120/400, running loss: 0.042\n",
      "vit_tiny - Current time: 635.0 s, epoch: 6/50, minibatch:   140/400, running loss: 0.042\n",
      "vit_tiny - Current time: 726.0 s, epoch: 6/50, minibatch:   160/400, running loss: 0.048\n",
      "vit_tiny - Current time: 818.0 s, epoch: 6/50, minibatch:   180/400, running loss: 0.046\n",
      "vit_tiny - Current time: 908.0 s, epoch: 6/50, minibatch:   200/400, running loss: 0.045\n",
      "vit_tiny - Current time: 998.0 s, epoch: 6/50, minibatch:   220/400, running loss: 0.045\n",
      "vit_tiny - Current time: 1089.0 s, epoch: 6/50, minibatch:   240/400, running loss: 0.042\n",
      "vit_tiny - Current time: 1179.0 s, epoch: 6/50, minibatch:   260/400, running loss: 0.043\n",
      "vit_tiny - Current time: 1270.0 s, epoch: 6/50, minibatch:   280/400, running loss: 0.042\n",
      "vit_tiny - Current time: 1360.0 s, epoch: 6/50, minibatch:   300/400, running loss: 0.042\n",
      "vit_tiny - Current time: 1451.0 s, epoch: 6/50, minibatch:   320/400, running loss: 0.045\n",
      "vit_tiny - Current time: 1542.0 s, epoch: 6/50, minibatch:   340/400, running loss: 0.048\n",
      "vit_tiny - Current time: 1632.0 s, epoch: 6/50, minibatch:   360/400, running loss: 0.044\n",
      "vit_tiny - Current time: 1722.0 s, epoch: 6/50, minibatch:   380/400, running loss: 0.044\n",
      "vit_tiny - Current time: 1812.0 s, epoch: 6/50, minibatch:   400/400, running loss: 0.042\n",
      "vit_tiny - Epoch 6: Train Loss = 1.1011, Val Loss = 1.0577, Val F1 = 0.6160\n",
      "vit_tiny - Current time: 91.0 s, epoch: 7/50, minibatch:    20/400, running loss: 0.040\n",
      "vit_tiny - Current time: 181.0 s, epoch: 7/50, minibatch:    40/400, running loss: 0.039\n",
      "vit_tiny - Current time: 271.0 s, epoch: 7/50, minibatch:    60/400, running loss: 0.040\n",
      "vit_tiny - Current time: 362.0 s, epoch: 7/50, minibatch:    80/400, running loss: 0.042\n",
      "vit_tiny - Current time: 452.0 s, epoch: 7/50, minibatch:   100/400, running loss: 0.041\n",
      "vit_tiny - Current time: 543.0 s, epoch: 7/50, minibatch:   120/400, running loss: 0.044\n",
      "vit_tiny - Current time: 634.0 s, epoch: 7/50, minibatch:   140/400, running loss: 0.043\n",
      "vit_tiny - Current time: 724.0 s, epoch: 7/50, minibatch:   160/400, running loss: 0.042\n",
      "vit_tiny - Current time: 815.0 s, epoch: 7/50, minibatch:   180/400, running loss: 0.041\n",
      "vit_tiny - Current time: 905.0 s, epoch: 7/50, minibatch:   200/400, running loss: 0.041\n",
      "vit_tiny - Current time: 997.0 s, epoch: 7/50, minibatch:   220/400, running loss: 0.040\n",
      "vit_tiny - Current time: 1087.0 s, epoch: 7/50, minibatch:   240/400, running loss: 0.041\n",
      "vit_tiny - Current time: 1178.0 s, epoch: 7/50, minibatch:   260/400, running loss: 0.039\n",
      "vit_tiny - Current time: 1269.0 s, epoch: 7/50, minibatch:   280/400, running loss: 0.042\n",
      "vit_tiny - Current time: 1360.0 s, epoch: 7/50, minibatch:   300/400, running loss: 0.040\n",
      "vit_tiny - Current time: 1451.0 s, epoch: 7/50, minibatch:   320/400, running loss: 0.041\n",
      "vit_tiny - Current time: 1542.0 s, epoch: 7/50, minibatch:   340/400, running loss: 0.038\n",
      "vit_tiny - Current time: 1632.0 s, epoch: 7/50, minibatch:   360/400, running loss: 0.041\n",
      "vit_tiny - Current time: 1722.0 s, epoch: 7/50, minibatch:   380/400, running loss: 0.040\n",
      "vit_tiny - Current time: 1813.0 s, epoch: 7/50, minibatch:   400/400, running loss: 0.038\n",
      "vit_tiny - Epoch 7: Train Loss = 1.0164, Val Loss = 1.0793, Val F1 = 0.6310\n",
      "vit_tiny - Current time: 90.0 s, epoch: 8/50, minibatch:    20/400, running loss: 0.042\n",
      "vit_tiny - Current time: 181.0 s, epoch: 8/50, minibatch:    40/400, running loss: 0.038\n",
      "vit_tiny - Current time: 272.0 s, epoch: 8/50, minibatch:    60/400, running loss: 0.035\n",
      "vit_tiny - Current time: 363.0 s, epoch: 8/50, minibatch:    80/400, running loss: 0.036\n",
      "vit_tiny - Current time: 453.0 s, epoch: 8/50, minibatch:   100/400, running loss: 0.040\n",
      "vit_tiny - Current time: 544.0 s, epoch: 8/50, minibatch:   120/400, running loss: 0.039\n",
      "vit_tiny - Current time: 635.0 s, epoch: 8/50, minibatch:   140/400, running loss: 0.041\n",
      "vit_tiny - Current time: 725.0 s, epoch: 8/50, minibatch:   160/400, running loss: 0.040\n",
      "vit_tiny - Current time: 816.0 s, epoch: 8/50, minibatch:   180/400, running loss: 0.037\n",
      "vit_tiny - Current time: 907.0 s, epoch: 8/50, minibatch:   200/400, running loss: 0.041\n",
      "vit_tiny - Current time: 998.0 s, epoch: 8/50, minibatch:   220/400, running loss: 0.039\n",
      "vit_tiny - Current time: 1089.0 s, epoch: 8/50, minibatch:   240/400, running loss: 0.039\n",
      "vit_tiny - Current time: 1180.0 s, epoch: 8/50, minibatch:   260/400, running loss: 0.040\n",
      "vit_tiny - Current time: 1270.0 s, epoch: 8/50, minibatch:   280/400, running loss: 0.034\n",
      "vit_tiny - Current time: 1361.0 s, epoch: 8/50, minibatch:   300/400, running loss: 0.040\n",
      "vit_tiny - Current time: 1452.0 s, epoch: 8/50, minibatch:   320/400, running loss: 0.039\n",
      "vit_tiny - Current time: 1543.0 s, epoch: 8/50, minibatch:   340/400, running loss: 0.036\n",
      "vit_tiny - Current time: 1633.0 s, epoch: 8/50, minibatch:   360/400, running loss: 0.037\n",
      "vit_tiny - Current time: 1724.0 s, epoch: 8/50, minibatch:   380/400, running loss: 0.035\n",
      "vit_tiny - Current time: 1814.0 s, epoch: 8/50, minibatch:   400/400, running loss: 0.038\n",
      "vit_tiny - Epoch 8: Train Loss = 0.9581, Val Loss = 1.0311, Val F1 = 0.6285\n",
      "vit_tiny - Current time: 90.0 s, epoch: 9/50, minibatch:    20/400, running loss: 0.033\n",
      "vit_tiny - Current time: 181.0 s, epoch: 9/50, minibatch:    40/400, running loss: 0.038\n",
      "vit_tiny - Current time: 271.0 s, epoch: 9/50, minibatch:    60/400, running loss: 0.036\n",
      "vit_tiny - Current time: 363.0 s, epoch: 9/50, minibatch:    80/400, running loss: 0.037\n",
      "vit_tiny - Current time: 454.0 s, epoch: 9/50, minibatch:   100/400, running loss: 0.039\n",
      "vit_tiny - Current time: 544.0 s, epoch: 9/50, minibatch:   120/400, running loss: 0.034\n",
      "vit_tiny - Current time: 634.0 s, epoch: 9/50, minibatch:   140/400, running loss: 0.038\n",
      "vit_tiny - Current time: 725.0 s, epoch: 9/50, minibatch:   160/400, running loss: 0.034\n",
      "vit_tiny - Current time: 816.0 s, epoch: 9/50, minibatch:   180/400, running loss: 0.036\n",
      "vit_tiny - Current time: 906.0 s, epoch: 9/50, minibatch:   200/400, running loss: 0.039\n",
      "vit_tiny - Current time: 997.0 s, epoch: 9/50, minibatch:   220/400, running loss: 0.034\n",
      "vit_tiny - Current time: 1087.0 s, epoch: 9/50, minibatch:   240/400, running loss: 0.037\n",
      "vit_tiny - Current time: 1178.0 s, epoch: 9/50, minibatch:   260/400, running loss: 0.033\n",
      "vit_tiny - Current time: 1268.0 s, epoch: 9/50, minibatch:   280/400, running loss: 0.038\n",
      "vit_tiny - Current time: 1358.0 s, epoch: 9/50, minibatch:   300/400, running loss: 0.036\n",
      "vit_tiny - Current time: 1449.0 s, epoch: 9/50, minibatch:   320/400, running loss: 0.036\n",
      "vit_tiny - Current time: 1539.0 s, epoch: 9/50, minibatch:   340/400, running loss: 0.036\n",
      "vit_tiny - Current time: 1633.0 s, epoch: 9/50, minibatch:   360/400, running loss: 0.035\n",
      "vit_tiny - Current time: 1731.0 s, epoch: 9/50, minibatch:   380/400, running loss: 0.035\n",
      "vit_tiny - Current time: 1828.0 s, epoch: 9/50, minibatch:   400/400, running loss: 0.030\n",
      "vit_tiny - Epoch 9: Train Loss = 0.8948, Val Loss = 0.9691, Val F1 = 0.6655\n",
      "vit_tiny - Current time: 95.0 s, epoch: 10/50, minibatch:    20/400, running loss: 0.031\n",
      "vit_tiny - Current time: 190.0 s, epoch: 10/50, minibatch:    40/400, running loss: 0.033\n",
      "vit_tiny - Current time: 286.0 s, epoch: 10/50, minibatch:    60/400, running loss: 0.029\n",
      "vit_tiny - Current time: 381.0 s, epoch: 10/50, minibatch:    80/400, running loss: 0.030\n",
      "vit_tiny - Current time: 477.0 s, epoch: 10/50, minibatch:   100/400, running loss: 0.032\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m ensemble_trainer \u001b[38;5;241m=\u001b[39m EnsembleTrainer(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train all models\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mensemble_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Perform ensemble prediction\u001b[39;00m\n\u001b[0;32m     14\u001b[0m ensemble_f1 \u001b[38;5;241m=\u001b[39m ensemble_trainer\u001b[38;5;241m.\u001b[39mensemble_predict(validation_loader)\n",
      "Cell \u001b[1;32mIn[8], line 212\u001b[0m, in \u001b[0;36mEnsembleTrainer.train_ensemble\u001b[1;34m(self, train_loader, validation_loader)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 140\u001b[0m, in \u001b[0;36mEnsembleTrainer.train_single_model\u001b[1;34m(self, model, optimizer, train_loader, validation_loader, model_name)\u001b[0m\n\u001b[0;32m    138\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    139\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[1;32m--> 140\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    143\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have train_loader and validation_loader defined\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "set_seed(23)\n",
    "\n",
    "train_loader, validation_loader = prepare_data_loaders((224,224), 1)\n",
    "\n",
    "ensemble_trainer = EnsembleTrainer(num_classes=8, device=device, max_epochs = 50)\n",
    "\n",
    "# Train all models\n",
    "ensemble_trainer.train_ensemble(train_loader, validation_loader)\n",
    "\n",
    "# Perform ensemble prediction\n",
    "ensemble_f1 = ensemble_trainer.ensemble_predict(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import torchvision.models as models\n",
    "import timm  # for Vision Transformer\n",
    "\n",
    "class EnsembleTrainer:\n",
    "    def __init__(self, num_classes, device, patience=10, min_delta=0.01, max_epochs=50):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        self.model_weights = dict()\n",
    "\n",
    "\n",
    "        self.models = {\n",
    "            'mnasnet1_3': self._prepare_mnasnet1_3(num_classes)\n",
    "        }\n",
    "        \n",
    "        # Optimizers for each model\n",
    "        self.optimizers = {\n",
    "            name: optim.AdamW(model.parameters()) \n",
    "            for name, model in self.models.items()\n",
    "        }\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.results = {\n",
    "            name: {\n",
    "                'train_losses': [],\n",
    "                'val_losses': [],\n",
    "                'f1_scores': [],\n",
    "                'best_val_f1': 0,\n",
    "                'patience_counter': 0\n",
    "            } for name in self.models.keys()\n",
    "        }\n",
    "\n",
    "    def _prepare_mnasnet1_3(self, num_classes):\n",
    "        model = models.mnasnet1_3(weights=\"DEFAULT\")\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _prepare_efficientnet_b0(self, num_classes):\n",
    "        model = models.efficientnet_b0(weights='DEFAULT')\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _prepare_resnet_18(self, num_classes):\n",
    "        model = models.resnet18(weights='DEFAULT')\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def _prepare_mobilenet_v3_large(self, num_classes):\n",
    "        model = models.mobilenet_v3_large(weights='IMAGENET1K_V2')\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _prepare_vit_tiny(self, num_classes):\n",
    "        model = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def _prepare_swin_t(self, num_classes):\n",
    "        model = models.swin_t(weights='DEFAULT')\n",
    "        if hasattr(model, 'head'):\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        else:\n",
    "            model.fc = nn.Linear(model.num_features, num_classes)\n",
    "        return model.to(self.device)\n",
    "        \n",
    "\n",
    "    def train_single_model(self, model, optimizer, train_loader, validation_loader, model_name):\n",
    "        for epoch in range(1, self.max_epochs + 1):  # Your original epoch range\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Training phase\n",
    "            model.train()\n",
    "            epoch_loss_train = 0\n",
    "            running_loss = 0\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss_train += loss.item()\n",
    "                \n",
    "                if i % 20 == 19:\n",
    "                    print(f\"{model_name} - Current time: {round(time.time() - current_time, 0)} s, \"\n",
    "                          f\"epoch: {epoch}/50, minibatch: {i + 1:5d}/{len(train_loader)}, \"\n",
    "                          f\"running loss: {running_loss / 500:.3f}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            epoch_loss_val = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in validation_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    epoch_loss_val += loss.item()\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Compute metrics\n",
    "            avg_train_loss = epoch_loss_train / len(train_loader)\n",
    "            avg_val_loss = epoch_loss_val / len(validation_loader)\n",
    "            val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "            # Update results\n",
    "            results = self.results[model_name]\n",
    "            results['train_losses'].append(avg_train_loss)\n",
    "            results['val_losses'].append(avg_val_loss)\n",
    "            results['f1_scores'].append(val_f1)\n",
    "\n",
    "            print(f\"{model_name} - Epoch {epoch}: \"\n",
    "                  f\"Train Loss = {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss = {avg_val_loss:.4f}, \"\n",
    "                  f\"Val F1 = {val_f1:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_f1 > results['best_val_f1'] + self.min_delta:\n",
    "                results['best_val_f1'] = val_f1\n",
    "                results['patience_counter'] = 0\n",
    "                \n",
    "                # Save best model\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': results['train_losses'],\n",
    "                    'val_loss': results['val_losses'],\n",
    "                    'f1_metric_val': results['f1_scores'],\n",
    "                    'best_val_f1': results['best_val_f1'],\n",
    "                }, f'models/{model_name}_best_f1.pth')\n",
    "                self.model_weights[model_name] = val_f1\n",
    "            else:\n",
    "                results['patience_counter'] += 1\n",
    "\n",
    "            if results['patience_counter'] >= self.patience:\n",
    "                print(f\"{model_name} - Early stopping triggered at epoch {epoch}, best f1 score {results['f1_scores'][-11]}\")\n",
    "                break\n",
    "\n",
    "    def train_ensemble(self, train_loader, validation_loader):\n",
    "        # Train each model\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nTraining {name}\")\n",
    "            self.train_single_model(model, self.optimizers[name], \n",
    "                                    train_loader, validation_loader, name)\n",
    "\n",
    "    def ensemble_predict(self, dataloader):\n",
    "        # Weighted voting ensemble prediction\n",
    "        all_ensemble_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Normalizacja wag, jeśli nie sumują się do 1\n",
    "        total_weight = sum(self.model_weights.values())\n",
    "        model_weights_normalized = {model: weight / total_weight for model, weight in self.model_weights.items()}\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Zbieramy predykcje każdego modelu pomnożone przez ich wagi\n",
    "            weighted_preds = []\n",
    "            for model_name, model in self.models.items():\n",
    "                probs = torch.softmax(model(images), dim=1)\n",
    "                weight = model_weights_normalized.get(model_name, 0.0)  # Domyślna waga 0, jeśli model nie ma wagi\n",
    "                weighted_preds.append(probs * weight)\n",
    "\n",
    "            # Suma ważonych predykcji\n",
    "            ensemble_probs = torch.sum(torch.stack(weighted_preds), dim=0)\n",
    "\n",
    "            # Ostateczne predykcje\n",
    "            _, ensemble_preds = torch.max(ensemble_probs, 1)\n",
    "\n",
    "            all_ensemble_preds.extend(ensemble_preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Obliczenie F1-score dla ostatecznych predykcji\n",
    "        ensemble_f1 = f1_score(all_labels, all_ensemble_preds, average='macro')\n",
    "        print(f\"Weighted Ensemble F1 Score: {ensemble_f1:.4f}\")\n",
    "\n",
    "        return ensemble_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mnasnet1_3-a4c69d6f.pth\" to C:\\Users\\dimpi/.cache\\torch\\hub\\checkpoints\\mnasnet1_3-a4c69d6f.pth\n",
      "100%|██████████| 24.2M/24.2M [00:03<00:00, 7.93MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training mnasnet1_3\n",
      "mnasnet1_3 - Current time: 12.0 s, epoch: 1/50, minibatch:    20/400, running loss: 0.051\n",
      "mnasnet1_3 - Current time: 22.0 s, epoch: 1/50, minibatch:    40/400, running loss: 0.031\n",
      "mnasnet1_3 - Current time: 33.0 s, epoch: 1/50, minibatch:    60/400, running loss: 0.030\n",
      "mnasnet1_3 - Current time: 44.0 s, epoch: 1/50, minibatch:    80/400, running loss: 0.028\n",
      "mnasnet1_3 - Current time: 54.0 s, epoch: 1/50, minibatch:   100/400, running loss: 0.028\n",
      "mnasnet1_3 - Current time: 64.0 s, epoch: 1/50, minibatch:   120/400, running loss: 0.023\n",
      "mnasnet1_3 - Current time: 75.0 s, epoch: 1/50, minibatch:   140/400, running loss: 0.022\n",
      "mnasnet1_3 - Current time: 85.0 s, epoch: 1/50, minibatch:   160/400, running loss: 0.025\n",
      "mnasnet1_3 - Current time: 96.0 s, epoch: 1/50, minibatch:   180/400, running loss: 0.020\n",
      "mnasnet1_3 - Current time: 106.0 s, epoch: 1/50, minibatch:   200/400, running loss: 0.022\n",
      "mnasnet1_3 - Current time: 116.0 s, epoch: 1/50, minibatch:   220/400, running loss: 0.023\n",
      "mnasnet1_3 - Current time: 126.0 s, epoch: 1/50, minibatch:   240/400, running loss: 0.020\n",
      "mnasnet1_3 - Current time: 136.0 s, epoch: 1/50, minibatch:   260/400, running loss: 0.022\n",
      "mnasnet1_3 - Current time: 146.0 s, epoch: 1/50, minibatch:   280/400, running loss: 0.022\n",
      "mnasnet1_3 - Current time: 156.0 s, epoch: 1/50, minibatch:   300/400, running loss: 0.021\n",
      "mnasnet1_3 - Current time: 166.0 s, epoch: 1/50, minibatch:   320/400, running loss: 0.022\n",
      "mnasnet1_3 - Current time: 175.0 s, epoch: 1/50, minibatch:   340/400, running loss: 0.018\n",
      "mnasnet1_3 - Current time: 185.0 s, epoch: 1/50, minibatch:   360/400, running loss: 0.020\n",
      "mnasnet1_3 - Current time: 194.0 s, epoch: 1/50, minibatch:   380/400, running loss: 0.019\n",
      "mnasnet1_3 - Current time: 204.0 s, epoch: 1/50, minibatch:   400/400, running loss: 0.017\n",
      "mnasnet1_3 - Epoch 1: Train Loss = 0.6021, Val Loss = 195.2620, Val F1 = 0.1672\n",
      "mnasnet1_3 - Current time: 9.0 s, epoch: 2/50, minibatch:    20/400, running loss: 0.018\n",
      "mnasnet1_3 - Current time: 19.0 s, epoch: 2/50, minibatch:    40/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 28.0 s, epoch: 2/50, minibatch:    60/400, running loss: 0.015\n",
      "mnasnet1_3 - Current time: 37.0 s, epoch: 2/50, minibatch:    80/400, running loss: 0.013\n",
      "mnasnet1_3 - Current time: 47.0 s, epoch: 2/50, minibatch:   100/400, running loss: 0.015\n",
      "mnasnet1_3 - Current time: 57.0 s, epoch: 2/50, minibatch:   120/400, running loss: 0.015\n",
      "mnasnet1_3 - Current time: 66.0 s, epoch: 2/50, minibatch:   140/400, running loss: 0.013\n",
      "mnasnet1_3 - Current time: 75.0 s, epoch: 2/50, minibatch:   160/400, running loss: 0.016\n",
      "mnasnet1_3 - Current time: 85.0 s, epoch: 2/50, minibatch:   180/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 95.0 s, epoch: 2/50, minibatch:   200/400, running loss: 0.016\n",
      "mnasnet1_3 - Current time: 104.0 s, epoch: 2/50, minibatch:   220/400, running loss: 0.017\n",
      "mnasnet1_3 - Current time: 114.0 s, epoch: 2/50, minibatch:   240/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 124.0 s, epoch: 2/50, minibatch:   260/400, running loss: 0.015\n",
      "mnasnet1_3 - Current time: 133.0 s, epoch: 2/50, minibatch:   280/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 142.0 s, epoch: 2/50, minibatch:   300/400, running loss: 0.017\n",
      "mnasnet1_3 - Current time: 152.0 s, epoch: 2/50, minibatch:   320/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 162.0 s, epoch: 2/50, minibatch:   340/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 171.0 s, epoch: 2/50, minibatch:   360/400, running loss: 0.017\n",
      "mnasnet1_3 - Current time: 181.0 s, epoch: 2/50, minibatch:   380/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 191.0 s, epoch: 2/50, minibatch:   400/400, running loss: 0.013\n",
      "mnasnet1_3 - Epoch 2: Train Loss = 0.3711, Val Loss = 202.5666, Val F1 = 0.0958\n",
      "mnasnet1_3 - Current time: 10.0 s, epoch: 3/50, minibatch:    20/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 19.0 s, epoch: 3/50, minibatch:    40/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 29.0 s, epoch: 3/50, minibatch:    60/400, running loss: 0.013\n",
      "mnasnet1_3 - Current time: 38.0 s, epoch: 3/50, minibatch:    80/400, running loss: 0.013\n",
      "mnasnet1_3 - Current time: 48.0 s, epoch: 3/50, minibatch:   100/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 57.0 s, epoch: 3/50, minibatch:   120/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 67.0 s, epoch: 3/50, minibatch:   140/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 76.0 s, epoch: 3/50, minibatch:   160/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 85.0 s, epoch: 3/50, minibatch:   180/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 95.0 s, epoch: 3/50, minibatch:   200/400, running loss: 0.013\n",
      "mnasnet1_3 - Current time: 104.0 s, epoch: 3/50, minibatch:   220/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 114.0 s, epoch: 3/50, minibatch:   240/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 123.0 s, epoch: 3/50, minibatch:   260/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 133.0 s, epoch: 3/50, minibatch:   280/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 142.0 s, epoch: 3/50, minibatch:   300/400, running loss: 0.014\n",
      "mnasnet1_3 - Current time: 152.0 s, epoch: 3/50, minibatch:   320/400, running loss: 0.015\n",
      "mnasnet1_3 - Current time: 161.0 s, epoch: 3/50, minibatch:   340/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 171.0 s, epoch: 3/50, minibatch:   360/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 180.0 s, epoch: 3/50, minibatch:   380/400, running loss: 0.013\n",
      "mnasnet1_3 - Current time: 190.0 s, epoch: 3/50, minibatch:   400/400, running loss: 0.011\n",
      "mnasnet1_3 - Epoch 3: Train Loss = 0.3119, Val Loss = 127.2783, Val F1 = 0.0675\n",
      "mnasnet1_3 - Current time: 10.0 s, epoch: 4/50, minibatch:    20/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 19.0 s, epoch: 4/50, minibatch:    40/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 29.0 s, epoch: 4/50, minibatch:    60/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 38.0 s, epoch: 4/50, minibatch:    80/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 48.0 s, epoch: 4/50, minibatch:   100/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 57.0 s, epoch: 4/50, minibatch:   120/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 67.0 s, epoch: 4/50, minibatch:   140/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 76.0 s, epoch: 4/50, minibatch:   160/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 86.0 s, epoch: 4/50, minibatch:   180/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 95.0 s, epoch: 4/50, minibatch:   200/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 104.0 s, epoch: 4/50, minibatch:   220/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 113.0 s, epoch: 4/50, minibatch:   240/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 122.0 s, epoch: 4/50, minibatch:   260/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 131.0 s, epoch: 4/50, minibatch:   280/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 140.0 s, epoch: 4/50, minibatch:   300/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 149.0 s, epoch: 4/50, minibatch:   320/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 158.0 s, epoch: 4/50, minibatch:   340/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 167.0 s, epoch: 4/50, minibatch:   360/400, running loss: 0.013\n",
      "mnasnet1_3 - Current time: 176.0 s, epoch: 4/50, minibatch:   380/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 185.0 s, epoch: 4/50, minibatch:   400/400, running loss: 0.011\n",
      "mnasnet1_3 - Epoch 4: Train Loss = 0.2658, Val Loss = 467.4403, Val F1 = 0.0278\n",
      "mnasnet1_3 - Current time: 9.0 s, epoch: 5/50, minibatch:    20/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 17.0 s, epoch: 5/50, minibatch:    40/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 26.0 s, epoch: 5/50, minibatch:    60/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 35.0 s, epoch: 5/50, minibatch:    80/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 44.0 s, epoch: 5/50, minibatch:   100/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 53.0 s, epoch: 5/50, minibatch:   120/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 62.0 s, epoch: 5/50, minibatch:   140/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 72.0 s, epoch: 5/50, minibatch:   160/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 81.0 s, epoch: 5/50, minibatch:   180/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 90.0 s, epoch: 5/50, minibatch:   200/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 99.0 s, epoch: 5/50, minibatch:   220/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 108.0 s, epoch: 5/50, minibatch:   240/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 117.0 s, epoch: 5/50, minibatch:   260/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 126.0 s, epoch: 5/50, minibatch:   280/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 135.0 s, epoch: 5/50, minibatch:   300/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 144.0 s, epoch: 5/50, minibatch:   320/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 154.0 s, epoch: 5/50, minibatch:   340/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 163.0 s, epoch: 5/50, minibatch:   360/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 171.0 s, epoch: 5/50, minibatch:   380/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 181.0 s, epoch: 5/50, minibatch:   400/400, running loss: 0.009\n",
      "mnasnet1_3 - Epoch 5: Train Loss = 0.2414, Val Loss = 609.9078, Val F1 = 0.0271\n",
      "mnasnet1_3 - Current time: 30.0 s, epoch: 6/50, minibatch:    20/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 67.0 s, epoch: 6/50, minibatch:    40/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 104.0 s, epoch: 6/50, minibatch:    60/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 139.0 s, epoch: 6/50, minibatch:    80/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 171.0 s, epoch: 6/50, minibatch:   100/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 180.0 s, epoch: 6/50, minibatch:   120/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 190.0 s, epoch: 6/50, minibatch:   140/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 199.0 s, epoch: 6/50, minibatch:   160/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 211.0 s, epoch: 6/50, minibatch:   180/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 222.0 s, epoch: 6/50, minibatch:   200/400, running loss: 0.011\n",
      "mnasnet1_3 - Current time: 234.0 s, epoch: 6/50, minibatch:   220/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 244.0 s, epoch: 6/50, minibatch:   240/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 255.0 s, epoch: 6/50, minibatch:   260/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 265.0 s, epoch: 6/50, minibatch:   280/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 276.0 s, epoch: 6/50, minibatch:   300/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 288.0 s, epoch: 6/50, minibatch:   320/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 298.0 s, epoch: 6/50, minibatch:   340/400, running loss: 0.012\n",
      "mnasnet1_3 - Current time: 307.0 s, epoch: 6/50, minibatch:   360/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 319.0 s, epoch: 6/50, minibatch:   380/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 328.0 s, epoch: 6/50, minibatch:   400/400, running loss: 0.007\n",
      "mnasnet1_3 - Epoch 6: Train Loss = 0.2279, Val Loss = 159.9753, Val F1 = 0.0591\n",
      "mnasnet1_3 - Current time: 12.0 s, epoch: 7/50, minibatch:    20/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 24.0 s, epoch: 7/50, minibatch:    40/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 34.0 s, epoch: 7/50, minibatch:    60/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 44.0 s, epoch: 7/50, minibatch:    80/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 53.0 s, epoch: 7/50, minibatch:   100/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 63.0 s, epoch: 7/50, minibatch:   120/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 73.0 s, epoch: 7/50, minibatch:   140/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 83.0 s, epoch: 7/50, minibatch:   160/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 92.0 s, epoch: 7/50, minibatch:   180/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 104.0 s, epoch: 7/50, minibatch:   200/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 115.0 s, epoch: 7/50, minibatch:   220/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 126.0 s, epoch: 7/50, minibatch:   240/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 135.0 s, epoch: 7/50, minibatch:   260/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 146.0 s, epoch: 7/50, minibatch:   280/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 157.0 s, epoch: 7/50, minibatch:   300/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 168.0 s, epoch: 7/50, minibatch:   320/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 178.0 s, epoch: 7/50, minibatch:   340/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 188.0 s, epoch: 7/50, minibatch:   360/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 197.0 s, epoch: 7/50, minibatch:   380/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 207.0 s, epoch: 7/50, minibatch:   400/400, running loss: 0.009\n",
      "mnasnet1_3 - Epoch 7: Train Loss = 0.2124, Val Loss = 1422.1680, Val F1 = 0.0815\n",
      "mnasnet1_3 - Current time: 10.0 s, epoch: 8/50, minibatch:    20/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 20.0 s, epoch: 8/50, minibatch:    40/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 29.0 s, epoch: 8/50, minibatch:    60/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 39.0 s, epoch: 8/50, minibatch:    80/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 50.0 s, epoch: 8/50, minibatch:   100/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 60.0 s, epoch: 8/50, minibatch:   120/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 71.0 s, epoch: 8/50, minibatch:   140/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 81.0 s, epoch: 8/50, minibatch:   160/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 92.0 s, epoch: 8/50, minibatch:   180/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 103.0 s, epoch: 8/50, minibatch:   200/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 114.0 s, epoch: 8/50, minibatch:   220/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 124.0 s, epoch: 8/50, minibatch:   240/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 135.0 s, epoch: 8/50, minibatch:   260/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 145.0 s, epoch: 8/50, minibatch:   280/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 155.0 s, epoch: 8/50, minibatch:   300/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 166.0 s, epoch: 8/50, minibatch:   320/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 176.0 s, epoch: 8/50, minibatch:   340/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 186.0 s, epoch: 8/50, minibatch:   360/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 220.0 s, epoch: 8/50, minibatch:   380/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 260.0 s, epoch: 8/50, minibatch:   400/400, running loss: 0.010\n",
      "mnasnet1_3 - Epoch 8: Train Loss = 0.2116, Val Loss = 134.9036, Val F1 = 0.0308\n",
      "mnasnet1_3 - Current time: 15.0 s, epoch: 9/50, minibatch:    20/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 27.0 s, epoch: 9/50, minibatch:    40/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 40.0 s, epoch: 9/50, minibatch:    60/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 52.0 s, epoch: 9/50, minibatch:    80/400, running loss: 0.005\n",
      "mnasnet1_3 - Current time: 64.0 s, epoch: 9/50, minibatch:   100/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 76.0 s, epoch: 9/50, minibatch:   120/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 88.0 s, epoch: 9/50, minibatch:   140/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 99.0 s, epoch: 9/50, minibatch:   160/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 111.0 s, epoch: 9/50, minibatch:   180/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 122.0 s, epoch: 9/50, minibatch:   200/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 134.0 s, epoch: 9/50, minibatch:   220/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 146.0 s, epoch: 9/50, minibatch:   240/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 157.0 s, epoch: 9/50, minibatch:   260/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 168.0 s, epoch: 9/50, minibatch:   280/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 180.0 s, epoch: 9/50, minibatch:   300/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 192.0 s, epoch: 9/50, minibatch:   320/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 204.0 s, epoch: 9/50, minibatch:   340/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 215.0 s, epoch: 9/50, minibatch:   360/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 227.0 s, epoch: 9/50, minibatch:   380/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 239.0 s, epoch: 9/50, minibatch:   400/400, running loss: 0.007\n",
      "mnasnet1_3 - Epoch 9: Train Loss = 0.1860, Val Loss = 72.5228, Val F1 = 0.0346\n",
      "mnasnet1_3 - Current time: 10.0 s, epoch: 10/50, minibatch:    20/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 21.0 s, epoch: 10/50, minibatch:    40/400, running loss: 0.005\n",
      "mnasnet1_3 - Current time: 31.0 s, epoch: 10/50, minibatch:    60/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 41.0 s, epoch: 10/50, minibatch:    80/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 51.0 s, epoch: 10/50, minibatch:   100/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 61.0 s, epoch: 10/50, minibatch:   120/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 72.0 s, epoch: 10/50, minibatch:   140/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 82.0 s, epoch: 10/50, minibatch:   160/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 92.0 s, epoch: 10/50, minibatch:   180/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 103.0 s, epoch: 10/50, minibatch:   200/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 113.0 s, epoch: 10/50, minibatch:   220/400, running loss: 0.005\n",
      "mnasnet1_3 - Current time: 124.0 s, epoch: 10/50, minibatch:   240/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 134.0 s, epoch: 10/50, minibatch:   260/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 145.0 s, epoch: 10/50, minibatch:   280/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 156.0 s, epoch: 10/50, minibatch:   300/400, running loss: 0.010\n",
      "mnasnet1_3 - Current time: 166.0 s, epoch: 10/50, minibatch:   320/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 177.0 s, epoch: 10/50, minibatch:   340/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 189.0 s, epoch: 10/50, minibatch:   360/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 199.0 s, epoch: 10/50, minibatch:   380/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 210.0 s, epoch: 10/50, minibatch:   400/400, running loss: 0.010\n",
      "mnasnet1_3 - Epoch 10: Train Loss = 0.1961, Val Loss = 139.9193, Val F1 = 0.0374\n",
      "mnasnet1_3 - Current time: 9.0 s, epoch: 11/50, minibatch:    20/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 19.0 s, epoch: 11/50, minibatch:    40/400, running loss: 0.004\n",
      "mnasnet1_3 - Current time: 28.0 s, epoch: 11/50, minibatch:    60/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 38.0 s, epoch: 11/50, minibatch:    80/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 47.0 s, epoch: 11/50, minibatch:   100/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 56.0 s, epoch: 11/50, minibatch:   120/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 66.0 s, epoch: 11/50, minibatch:   140/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 76.0 s, epoch: 11/50, minibatch:   160/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 85.0 s, epoch: 11/50, minibatch:   180/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 96.0 s, epoch: 11/50, minibatch:   200/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 106.0 s, epoch: 11/50, minibatch:   220/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 116.0 s, epoch: 11/50, minibatch:   240/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 126.0 s, epoch: 11/50, minibatch:   260/400, running loss: 0.006\n",
      "mnasnet1_3 - Current time: 137.0 s, epoch: 11/50, minibatch:   280/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 148.0 s, epoch: 11/50, minibatch:   300/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 158.0 s, epoch: 11/50, minibatch:   320/400, running loss: 0.009\n",
      "mnasnet1_3 - Current time: 167.0 s, epoch: 11/50, minibatch:   340/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 176.0 s, epoch: 11/50, minibatch:   360/400, running loss: 0.008\n",
      "mnasnet1_3 - Current time: 186.0 s, epoch: 11/50, minibatch:   380/400, running loss: 0.007\n",
      "mnasnet1_3 - Current time: 195.0 s, epoch: 11/50, minibatch:   400/400, running loss: 0.009\n",
      "mnasnet1_3 - Epoch 11: Train Loss = 0.1800, Val Loss = 37.1027, Val F1 = 0.1012\n",
      "mnasnet1_3 - Early stopping triggered at epoch 11, best f1 score 0.09579811193518267\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m ensemble_trainer\u001b[38;5;241m.\u001b[39mtrain_ensemble(train_loader, validation_loader)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Perform ensemble prediction\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m ensemble_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensemble_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 181\u001b[0m, in \u001b[0;36mEnsembleTrainer.ensemble_predict\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m    179\u001b[0m weighted_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 181\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    182\u001b[0m     weight \u001b[38;5;241m=\u001b[39m model_weights_normalized\u001b[38;5;241m.\u001b[39mget(model_name, \u001b[38;5;241m0.0\u001b[39m)  \u001b[38;5;66;03m# Domyślna waga 0, jeśli model nie ma wagi\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     weighted_preds\u001b[38;5;241m.\u001b[39mappend(probs \u001b[38;5;241m*\u001b[39m weight)\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torchvision\\models\\mnasnet.py:159\u001b[0m, in \u001b[0;36mMNASNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 159\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Equivalent to global avgpool and removing H and W dimensions.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torchvision\\models\\mnasnet.py:61\u001b[0m, in \u001b[0;36m_InvertedResidual.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_residual:\n\u001b[1;32m---> 61\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dimpi\\Desktop\\Hackathon\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have train_loader and validation_loader defined\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "set_seed(23)\n",
    "\n",
    "train_loader, validation_loader = prepare_data_loaders((224,224), 1)\n",
    "\n",
    "ensemble_trainer = EnsembleTrainer(num_classes=8, device=device, max_epochs = 50)\n",
    "\n",
    "# Train all models\n",
    "ensemble_trainer.train_ensemble(train_loader, validation_loader)\n",
    "\n",
    "# Perform ensemble prediction\n",
    "ensemble_f1 = ensemble_trainer.ensemble_predict(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StackingMetaLearner(nn.Module):\n",
    "    def __init__(self, num_input_models, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize the meta-learner neural network for stacking.\n",
    "        \n",
    "        Args:\n",
    "        - num_input_models (int): Number of base models feeding into this meta-learner\n",
    "        - num_classes (int): Number of output classes to predict\n",
    "        \"\"\"\n",
    "        super(StackingMetaLearner, self).__init__()\n",
    "        \n",
    "        # Define the network architecture\n",
    "        self.layers = nn.Sequential(\n",
    "            # Input layer: each input is the prediction probability from a base model\n",
    "            nn.Linear(num_input_models * num_classes, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Hidden layers\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Dropout for regularization\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the meta-learner\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Concatenated predictions from base models\n",
    "                           Shape: [batch_size, num_models * num_classes]\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Final predictions\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stacking_input(base_model_predictions):\n",
    "    \"\"\"\n",
    "    Prepare input for the meta-learner by concatenating predictions from base models\n",
    "    \n",
    "    Args:\n",
    "    base_model_predictions (list of torch.Tensor): \n",
    "        List of prediction tensors from each base model\n",
    "        Each tensor should have shape [batch_size, num_classes]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Concatenated predictions ready for meta-learner input\n",
    "    \"\"\"\n",
    "    # Ensure all prediction tensors are on the same device\n",
    "    base_model_predictions = [pred.cpu() for pred in base_model_predictions]\n",
    "    \n",
    "    # Concatenate predictions along the last dimension\n",
    "    stacked_input = torch.cat(base_model_predictions, dim=1)\n",
    "    \n",
    "    return stacked_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleTrainer:\n",
    "    def __init__(self, num_classes, device):\n",
    "        self.device = device\n",
    "        self.stacking_model = StackingMetaLearner(6, 8)\n",
    "        self.stacking_model = self.stacking_model.to(device)\n",
    "        self.model_weights = dict()\n",
    "        self.models = {\n",
    "            #'swin_t': self._prepare_swin_t(num_classes, 'swin_t'),\n",
    "            'efficientnet_b0': self._prepare_efficientnet_b0(num_classes, 'efficientnet_b0'),\n",
    "            'resnet_18': self._prepare_resnet_18(num_classes, 'resnet_18'),\n",
    "            'mobilenet_v3_large': self._prepare_mobilenet_v3_large(num_classes, 'mobilenet_v3_large'),\n",
    "            #'vit_tiny': self._prepare_vit_tiny(num_classes, 'vit_tiny')\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # Optimizers for each model\n",
    "        self.stacking_optimizers = optim.AdamW(self.stacking_model.parameters())\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.stacking_results = {\n",
    "                'train_losses': [],\n",
    "                'val_losses': [],\n",
    "                'f1_scores': [],\n",
    "                'best_val_f1': 0,\n",
    "                'patience_counter': 0\n",
    "        }\n",
    "\n",
    "\n",
    "    def _prepare_efficientnet_b0(self, num_classes, model_name):\n",
    "        model = models.efficientnet_b0(weights='DEFAULT')\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "        # Load the saved checkpoint\n",
    "        checkpoint = torch.load(f'models/{model_name}_best_f1.pth')\n",
    "\n",
    "        # Load the model weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Optional: Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        self.model_weights[model_name] = checkpoint['best_val_f1']\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _prepare_resnet_18(self, num_classes, model_name):\n",
    "        model = models.resnet18(weights='DEFAULT')\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        \n",
    "        # Load the saved checkpoint\n",
    "        checkpoint = torch.load(f'models/{model_name}_best_f1.pth')\n",
    "\n",
    "        # Load the model weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Optional: Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        self.model_weights[model_name] = checkpoint['best_val_f1']\n",
    "        \n",
    "        return model.to(self.device)\n",
    "\n",
    "    def _prepare_mobilenet_v3_large(self, num_classes, model_name):\n",
    "        model = models.mobilenet_v3_large(weights='IMAGENET1K_V2')\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "        \n",
    "        # Load the saved checkpoint\n",
    "        checkpoint = torch.load(f'models/{model_name}_best_f1.pth')\n",
    "\n",
    "        # Load the model weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Optional: Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        self.model_weights[model_name] = checkpoint['best_val_f1']\n",
    "        \n",
    "        \n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _prepare_vit_tiny(self, num_classes, model_name):\n",
    "        model = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=num_classes)\n",
    "        # Load the saved checkpoint\n",
    "        checkpoint = torch.load(f'models/{model_name}_best_f1.pth')\n",
    "\n",
    "        # Load the model weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Optional: Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        self.model_weights[model_name] = checkpoint['best_val_f1']\n",
    "        \n",
    "        return model.to(self.device)\n",
    "\n",
    "    def _prepare_swin_t(self, num_classes, model_name):\n",
    "        model = models.swin_t(weights='DEFAULT')\n",
    "        if hasattr(model, 'head'):\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        else:\n",
    "            model.fc = nn.Linear(model.num_features, num_classes)\n",
    "            \n",
    "        # Load the saved checkpoint\n",
    "        checkpoint = torch.load(f'models/{model_name}_best_f1.pth')\n",
    "\n",
    "        # Load the model weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # Optional: Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        self.model_weights[model_name] = checkpoint['best_val_f1']\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def train_single_model(self, train_loader, validation_loader, model_name):\n",
    "        for epoch in range(1, 50 + 1):  # Your original epoch range\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Training phase\n",
    "            self.stacking_model.train()\n",
    "            epoch_loss_train = 0\n",
    "            running_loss = 0\n",
    "            weighted_preds = []\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                weighted_preds = []\n",
    "                for model_name, model in self.models.items():\n",
    "                    probs = torch.softmax(model(inputs), dim=1)\n",
    "                    weighted_preds.append(probs.cpu())\n",
    "                \n",
    "                \n",
    "                stacked_input = torch.cat(weighted_preds, dim=1)\n",
    "                self.stacking_optimizers.zero_grad()\n",
    "                outputs = self.stacking_model(stacked_input)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.stacking_optimizers.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss_train += loss.item()\n",
    "                \n",
    "                if i % 20 == 19:\n",
    "                    print(f\"{model_name} - Current time: {round(time.time() - current_time, 0)} s, \"\n",
    "                          f\"epoch: {epoch}/50, minibatch: {i + 1:5d}/{len(train_loader)}, \"\n",
    "                          f\"running loss: {running_loss / 500:.3f}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "            # Validation phase\n",
    "            self.stacking_model.eval()\n",
    "            epoch_loss_val = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in validation_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    weighted_preds = []\n",
    "                    for model_name, model in self.models.items():\n",
    "                        probs = torch.softmax(model(images), dim=1)\n",
    "                        weighted_preds.append(probs.cpu())\n",
    "                    \n",
    "                    \n",
    "                    stacked_input = torch.cat(weighted_preds, dim=1)\n",
    "                    outputs = self.stacking_model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    epoch_loss_val += loss.item()\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Compute metrics\n",
    "            avg_train_loss = epoch_loss_train / len(train_loader)\n",
    "            avg_val_loss = epoch_loss_val / len(validation_loader)\n",
    "            val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "            # Update results\n",
    "            self.stacking_results['train_losses'].append(avg_train_loss)\n",
    "            self.stacking_results['val_losses'].append(avg_val_loss)\n",
    "            self.stacking_results['f1_scores'].append(val_f1)\n",
    "\n",
    "            print(f\"{model_name} - Epoch {epoch}: \"\n",
    "                  f\"Train Loss = {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss = {avg_val_loss:.4f}, \"\n",
    "                  f\"Val F1 = {val_f1:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_f1 > self.stacking_results['best_val_f1'] + self.min_delta:\n",
    "                self.stacking_results['best_val_f1'] = val_f1\n",
    "                self.stacking_results['patience_counter'] = 0\n",
    "                \n",
    "                # Save best model\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.stacking_model.state_dict(),\n",
    "                    'optimizer_state_dict': self.stacking_optimizers.state_dict(),\n",
    "                    'train_loss': self.stacking_results['train_losses'],\n",
    "                    'val_loss': self.stacking_results['val_losses'],\n",
    "                    'f1_metric_val': self.stacking_results['f1_scores'],\n",
    "                    'best_val_f1': self.stacking_results['best_val_f1'],\n",
    "                }, f'models/{model_name}_best_f1.pth')\n",
    "                self.model_weights[model_name] = val_f1\n",
    "            else:\n",
    "                self.stacking_results['patience_counter'] += 1\n",
    "\n",
    "            if self.stacking_results['patience_counter'] >= self.patience:\n",
    "                print(f\"{model_name} - Early stopping triggered at epoch {epoch}, best f1 score {self.stacking_results['f1_scores'][-11]}\")\n",
    "                break\n",
    "        \n",
    "\n",
    "    def ensemble_predict(self, dataloader):\n",
    "        # Weighted voting ensemble prediction\n",
    "        all_ensemble_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Normalizacja wag, jeśli nie sumują się do 1\n",
    "        total_weight = sum(self.model_weights.values())\n",
    "        model_weights_normalized = {model: weight / total_weight for model, weight in self.model_weights.items()}\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            print(\"Percent of complition:\", len(images)/len(dataloader))\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Zbieramy predykcje każdego modelu pomnożone przez ich wagi\n",
    "            weighted_preds = []\n",
    "            for model_name, model in self.models.items():\n",
    "                probs = torch.softmax(model(images), dim=1)\n",
    "                weight = model_weights_normalized.get(model_name, 0.0)  # Domyślna waga 0, jeśli model nie ma wagi\n",
    "                weighted_preds.append(probs * weight)\n",
    "\n",
    "            # Suma ważonych predykcji\n",
    "            ensemble_probs = torch.sum(torch.stack(weighted_preds), dim=0)\n",
    "\n",
    "            # Ostateczne predykcje\n",
    "            _, ensemble_preds = torch.max(ensemble_probs, 1)\n",
    "\n",
    "            all_ensemble_preds.extend(ensemble_preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Obliczenie F1-score dla ostatecznych predykcji\n",
    "        ensemble_f1 = f1_score(all_labels, all_ensemble_preds, average='macro')\n",
    "        print(f\"Weighted Ensemble F1 Score: {ensemble_f1}\")\n",
    "\n",
    "        return ensemble_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have train_loader and validation_loader defined\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "set_seed(23)\n",
    "\n",
    "train_loader, validation_loader = prepare_data_loaders((224,224), 1)\n",
    "\n",
    "ensemble_trainer = EnsembleTrainer(num_classes=8, device=device, max_epochs = 50)\n",
    "\n",
    "# Train all models\n",
    "ensemble_trainer.train_single_model(train_loader, validation_loader, 'stacking_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
